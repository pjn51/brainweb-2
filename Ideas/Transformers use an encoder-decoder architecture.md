*Transformers use an encoder-decoder architecture.* The [[transformer]] [[neural network]] architecture uses encoder layers that process the input, and decoder layers that process the output of the previous encoder layer. 

The purpose of each encoder layer is to generate encodings that contain info about which parts of the input are relevant to each other. The decoder does the opposite, moving from the encoder's info to an output sequence. 

#idea/compsci/data-science 

---
[1]: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)