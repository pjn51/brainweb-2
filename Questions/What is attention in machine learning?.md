*What is attention in machine learning?* [[attention mechanisms|Attention]] is a [[machine learning]] technique that the [[transformer]] architecture is based on. 

The idea is to mimic the human concept of attention - the effect assigns variable weights to the input data that reflect how important that datum is deemed to be in creating the output. The process of assigning weights can be done through [[gradient descent]]. [1]

#question/data-science 

---
[1]: https://en.wikipedia.org/wiki/Attention_(machine_learning)