# MLCon
#meeting/career

---
## Keynote: Building an operating system for AI
- Introductions
	- Maya: marketing dir at cnvrg.io (stage 1 host)
	- Leah Kolben: CEO of cnvrg.io
	- Yochay Ettun: CEO of cnvrg.io?
- What is an AI:OS?
	- Both these people did consulting in the [[data science industry]]
	- They saw need for platform to manage models
	- Noticed that time is wasted on infrastructure
	- They built cnvrg as an internal product
	- AI infrastructure is important
		- CPU
		- GPU
		- Accelerators
		- Cloud / on premises / hybrid hosting?
	- AI development is next level of complexity
		- They have methodologies on how to build, test, and deploy models
		- Different from web dev
			- Need to worry about data
			- Must create test and train
			- Tweaking [[hyperparameters]]
			- How to deploy model ([[communication and deployment]])
	- [[machine learning]] becoming more central --> more users involved
		- More executives and engineers getting into the process
	- cnvrg =multi level AI:OS
		- Managing resources
		- Managing [[modeling]]
		- Managing users
- Next generation AI:OS
	- They created *Papers*
	- To allow sharing and collab
	- Allows quick comparison and analysis of other [[data science]] work
	- Allows fast creation of research papers, including widgets that show code directly from your project on cnvrg. 
- Priority and queues
	- Need for tool to optimize time spent by Data Scientists
	- Utilize 100% of compute resources at all times
	- Improve productivity
	- This tool --> Queues by cnvrg

## Building attention recommendation at Wish
- Speaker: Chandler Phelps
- When Wish wants insight they build a dashboard
- This misses things
- Company issues
	- Low customer density
	- This means data as complex at Amazon
	- But fewer employees than a company like Amazon
- Allocating attention
	- We want to prioritize looking at abnormal events or patterns
	- This helps when there aren't enough people to be looking at all the data
	- Final goal is importance score, made up of...
		- Distance from expected value ([[z-score]] basically)
			- Getting the expectation is the hard part
			- Make sure this modeling task is done well and returns something that we can use as [[standard error]]
		- Size of the market segment affected (higher score for important markets)
			- They want importance to go up the larger the % of total business that the segment makes up
			- They define size through the metric of how much data they have for that segment
			- This overlooks if data for a segment dropped to zero, so we can use rolling average of the last week instead of just today numbers
		- Persistence of the anomaly over time
			- If an metric has periods of interest, that might be missed by a single threshold for alerts.
			- We can do a rolling sum to see if the metric is elevated for a period of time, and normalize this for the window that we're looking at
			- This allows us to detect persistent anomalies that never quite get above the reporting threshold at any single point

## Landing the first use cases of data science in a traditional non-tech enterprise
- Mantra is think big, learn fast, start small. 

## Making sense of ML metrics
- Speaker: Michael McCourt, head of engineering at SigOpt (an Intel company)
- Model building is an experiment
- Designing ML models for production
	- SigOpt powers the training process of mimimizing the [[loss function]], and the tuning process of tweaking [[hyperparameters]]. 
	- SigOpt introduces practical business concerns into these processes
- Classes of AI metrics
	- Training metrics
		- Decisions to be made:
			- Data source
			- Cleaning
			- [[feature engineering]]
			- Augmentation
			- More...
			- Choose the model
			- Choose the loss function <-- training metric
			- Optimize the model to minimize loss func
	- [[model validation|Validation]] metrics
		- Estimation of model performance on unseen data
		- Many options but more costly to optimize than training metrics
	- Guardrail metrics
		- Talked about less in the academic community
		- Represents practical limits of the modeling process
	- Production metrics
		- Post-deployment
- Summary
	- Design
	- Explore
	- Optimize
