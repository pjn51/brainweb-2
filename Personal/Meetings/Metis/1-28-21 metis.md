# Metis notes for Thursday, Jan 28th
`LINKS:` [[metis week 4]]
`TAGS:` #meeting/career

---
Today we are starting off with a pair programming exercise dealing with [[recursion]]. We also have lectures on Bayes' theorem and [[feature engineering]] in the afternoon. 

## Pair programming
We had to make a function that returned *n* factorial, and another that returned the *nth* number in the fibbonacci sequence. We were encouraged to use [[recursion]] for both functions. 

These functions also run into memory issues, especially the fibbonacci one at very high numbers. 

## Investigation - [[Jupyter Notebook]] tips
- Jupyter Notebook has useful extensions that can be found using nbextensions in the metis environment. 
- We can alt drag to create multiple cursors.
- We can use `@interact` to create interactive plots and things like that.

## Lecture - [[Bayes theorem]]

$$P(A|B)=\frac{P(B|A)* P(A)}{P(B)}$$

Bayes theorem is an important part of *bayesian [[statistics]],* and we will learn what the theorem is, as well as what it means for us.

Thomas Bayes was a statistician in the mid 1700s. His main idea was that the past informs the future, and that we can better infer the future based on past events. For example, if somebody tests positive for TB, we can use this idea to infer the [[probability]] that they *really* have the disease.

We can imagine a universe with a lot of people. In this universe, group A has the disease. The probability of us choosing a person at random and them having the disesase is $\frac{A}{universe}$.

There is another group, group B, who tested positive for the disease. Based on the metrics of the test, there might be a large or small overlap between these two groups. 

The joint probability *P(A, B)* is the probability that the person has the disease *and* tested positive for the disease. On the other hand, the probability that somebody who tested positive *actually has the disease* is defined as *P(A | B)*.

The probability that A will be true (they have the disease) given that B is true (they tested positive) is defined as...

$$P(A|B) = \frac {P(A,B)} {P(B)}$$

We can rewrite the above equation as...

$$P(A|B) = \frac {P(B,A) \cdot P(A)} {P(B)}$$

- The posterior probability: $P(A|B$)
- Prior probability: $P(A)$. This is just the overall probability that A would occur, based on past knowledge. hat A would occur, based on past knowledge. 
- Evidence: $P(B)$. This is based on what we've seen in the past. 

We could think of it as a ratio if we wanted to...

$$\frac{P(A)}{P(B)} = \frac{P(A|B)}{P(B|A)} $$

There are lots of applications for this theorem. Lots of [[machine learning]] models use it for prediction.

## Lecture - [[naive bayes]] classification
- Joint probability
	- $P(AB$)
	- We can expand these using the ==chain rule==
	- $P(AB) = P(A|B)\cdot P(B)$
	- $P(ABC) = P(A|BC) \cdot P(B|C) \cdot P(C)$
	- etc...
	- Think of each of these as an observation
	- P(AB) requires 2 calculations, P(ABC) needs three, etc. 
	- But these are only 3 of the 16 possible calculations (given that we have 4 features), why is this?
- Scaling conditional probability
	- With *k* [[binary]] features, we have $2^k$ possible combinations. 
	- For each, we need sufficent evidence
		- AKA each combination must occur enough times to see a pattern.
	- Imagine how many calculations / combinations there are for models with many features! 
- Independence
	- If we assume [[independence (probability)]] between features, we don't need as many calculations. 
	- $P(AB) = P(A) \cdot P(B)$
	- $P(ABC) =  P(A) \cdot P(B) \cdot P(C)$
	- This means we only need *k* calculations with sufficient [[data]]. 
- Conditional independence
	- Maybe the features are only independent given some outcome *C*. 
	- $P(AB|C)=P(A|C)\cdot P(B|C)$
- So what is naive bayes???
	- Capital letters represent random variables. Lowecase letters represent actual instances of the random distribution.
	- We want the probability that Y = *y*, given features *x*. 
	- $P(Y=y|X=x)$ or $P(y|x)$
	- If we assume independent features, how can we calculate the probability?
	- We treat this as our posterior probability from bayes theorem. 
	- We have to assume that $P(x)$ is constant for all observations of *x*. Meaning that each value of x is equally likely. 
	- We can erase the evidence portion since that doesnt change
	- We can further rewrite if we assume conditional independence
	- $P(x_1|y) \cdot P(x_2|y) \cdot ... \cdot P(x_k|y)$
	- For each x this becomes a maximum likelihood problem
	- We let [[Python]] handle the prior probability. 
	- For each x, we get a new input, and say for this x, whats the likelihood of y being each possible result? Then we maximize that likelihood.
- Another example
	- Given features A, B, and C, what label for D (0 or 1)?
	- Reverse the question. If D is 1, what is likelihood of seeing A, B, and C?
- Types
	- Bernoilli: binary features
	- Multinomial: features from multinomial distribution
	- Gasussian: features are continuous
	- Categorical: each feature is categorical
- Model considerations
	- We assume conditional independence of all features from all other features.
	- Independence doesn't have to be perfect, but features have to be *roughly* independent. 
	- Observations also have to be *roughly* independent.
	- More diverse data will lead to more evidence, which means stronger results. 
- Coding application
	- See my notebook `naive_bayes_three_examples.ipynb` for the implementation. 

## Lecture - [[feature engineering]] for classification
- Overview
	- We will see some examples of combining different datasets that seem unconnected for the purpose of classification. 
	- We can also engineer features from a small number of columns. 
- Example
	- For this case study, we will be looking at Instacart data
	- We will try and predict which products will appear in a user's next order given their order history. 
	- We want to make individual predictions for each user-product pair. 
- The best way to deal with lots of tables and feature engineering is to have one main table you work on. 
	- Put the most central info on that dataframe, and then we can add things to it and see what happens. 
- F1 is a good score to use when you have a large class inbalance. 