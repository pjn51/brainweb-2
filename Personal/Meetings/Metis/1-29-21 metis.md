# Metis Notes for Friday, 1-29-21
`TAGS:` #meeting/career

---
> [!info]
> Today is the final day of [[metis week 4]].

## Pair programming
Today is all about [[NumPy]]. See my notebook, `numpy.ipynb` in `onl_ds5/pairs`

==1: Sorting==
Suppose we have five students, 
`s = np.array(['Hannah', 'Astrid', 'Abdul', 'Mauve', 'Jung'])`, and test scores for each student, `t = np.array([99, 71, 85, 62, 91])`, respectively.

Given `s` and `t`, how can we calculate the name of the student with the lowest score using one line? What about the highest?
```python
s = np.array(['Hannah', 'Astrid', 'Abdul', 'Mauve', 'Jung'])
t = np.array([99, 71, 85, 62, 91])

s[t.argmin()]
```

Given `s` and `t`, how can we calculate the name of the student with the second highest score using one line?
```python
s[t.argsort()][-2]
```

Write the line of code that will list the names of students in _descending_ order of test score (i.e., a list of names, with associated scores in order from highest to lowest).
```python
s[t.argsort()[::-1]]
```

==2: Broadcasting==
Let's say I have a one dimensional array `x = np.array([1, 2, 3, 4, 5])`, and I want to make another array (with shape `(2, 5)`) that captures each of these values multiplied by `3` and `4`, respectively, for each row.
```python
x = np.array([1, 2, 3, 4, 5])

c = np.array([[3],[4]])

c*x
```

==3: Weighted Averages==
Now, let's say we want to calculate the average value for each row, but we really want weighted averages, where each column abides by the weights `w = np.array([1, 4, 1, 2, 5])`. How can we do this using one [[NumPy]] function?
```python
x = np.array([[ 3,  6,  9, 12, 15],
              [ 4,  8, 12, 16, 20]])

w = np.array([1, 4, 1, 2, 5])

y = np.array([ [np.mean(x[0]),np.mean(x[0]),np.mean(x[0]),np.mean(x[0]),np.mean(x[0])],
			[np.mean(x[1]),np.mean(x[1]),np.mean(x[1]),np.mean(x[1]),np.mean(x[1])] ])

y*w
```

Alternatively, we could use `np.average()`
```python
np.average(x, axis=1, weights=w)
```

## Lecture - [[complexity]] and classification models
For a code implementation, see  `model_complexity.ipynb` in `onl_ds5` repo.

It's important to know how models scale with the size of our [[data]], specifically as we add rows of data. We would like for our models to have less-than-linear [[complexity]], but linear is also great. Having less-than-linear complexity would mean that doubling the number of observations wouldn't double the time it took the model to work. 

We have to discern between *training complexity* and *testing complexity.* Some models take a really long time to train, but are very fast to make predictions with. Others are the opposite. For some situations, we might care a lot more about one or another, or both.

[[KNN]] has a very small train-time. This is because it just memorizes the dataset basically. However, it has a longer test-time since it has to analyze the distance between the new observation and every existing observation.

[[logistic regression]] has a less-than-linear time complexity for both training and testing.

Models that use [[gradient boosting]] have *exponential* train-time complexity as we add features, which is very bad. However, durin testing, they have linear complexity. 

[[radial SVM]] has a train-time that is completley unaffected by adding additional features, which is neat.

In general, more featues won't be an issue when predicting. But sometimes during training some models struggle with a large number of features. Usually, we're willing to take a hit on train-time if it means we save time during testing and deployment.

## Lecture - model linearity
- See my notebook, `Model_Linearity.ipynb` in the `onl_ds5` repo.
- A [[linear transformation]] or operation is a transformation that maintains or preserves scalar multiplication and addition.
	- As *a* increases, so does *b*, in a constant way. 
- In data sci we represent linear models as matrices, since we usually have so many x-variables that writing in the form $y=mx+b$ would be useless. 
- We turn coefficients into a ==weight vector== and multiply it by a matrix filled with all the x variables. 
- Every row of the reulting matrix is a linear combination of the weight vector and our data. This means we have a ==system of equations== on our hands.
- We can also use multiple weight vectors, resulting in multiple result vectors. 
- Takeaway: matrix multiplication is a bunch of linear transformations. 
- We use [[matrices]] as book-keeping tools basically to keep track of a lot of different linear transformations
- Linear and nonlinear
	- Many functions aren't linear. 
	- ==Kernel==:
		-  in LinAlg, represents what we have to do to get zero. In other senses, kernel can represent the intermediary function. 
		- We apply a non-linear thing to the x-variables, and then perform linear transformations within the matrix. 
		- Side note: [[logistic regression]] is performing a reverse kernel operation. We perform a linear operation and then a non-linear transformation of the result. 
	- We could introduce a kernel, or we could use strictly non-linear algorithms, like KNN or tree-based models. These algorithms don't depend on matrix-vector multiplication. 
	- Decision boundaries
		- Classification divides the world into decision areas, separated by boundaries that we have to find. 
		- If groups can be separated by a straight line, they are ==linearly seperable==. 
		- Using a linear function, we create a dividing line and classify points by seeing if they are above or below the line. 
		- Again, this only works if the data is linearlly seperable. Most of the time this is a dangerous assumption to make without investigation. 
- Models and linearity
	- [[radial SVM]] is non-linear.
	- [[naive bayes]] is non-linear. 
	- [[linear SVM]] is linear.
	- [[random forest]] is non-linear.
	- [[logistic regression]] is linear. 