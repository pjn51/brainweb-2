# Metis Notes for Wednesday, Jan 27th
`LINKS:` [[metis week 4]]
`TAGS:` #meeting/career

---
Today we're starting with a pair programming exercise, and then we have a lecture on classification metrics and a career workshop. Later in the day, we have another lecture on postgres. Our final ideas for project 3 are due today. 

## Pair programming
A version of this problem was faced at a whiteboard by a Metis student in an interview for a [[data]] scientist position on April 16, 2015.

In programming languages, there can be a lot of parentheses. The parentheses have to be "balanced" to be valid. For example, ()(()) is balanced, but ()()) is not balanced. Also, )((()) is not balanced.

Write a function that takes a string and returns True if the string's parentheses are balanced, False if they are not.

Our solutions: 
```python
def par1(mystr):
    in_list = list(char for char in mystr)    
    while len(in_list) > 0:
        if in_list[0]+in_list[-1] != '()':
            return False       
        else:
            in_list = in_list[1:-1]        
    return True

def par2(mystr):
    in_list = list(char for char in mustr)
    count = 0	
    for char in in_list:
        if char == '(':
            count += 1
        if char == ')':
            count -=1
        if count < 0:
            return False    
    if count == 0:
        return True    
    else:
        return False  
```

The second solution was slightly faster. Here is another solution:

```python
par3(mystr):
	mystr = re.sub('[^()]','', mystr) #remove all non parentheses
	if '()' in mystr:
		return par3(mystr.replace('()',''))
	else:
		return not mystr
```

## Lecture - [[classification metrics]]
Our objective here is to touch on a lot of metrics, and learn when to apply them.

[[Accuracy is an insufficient classification metric]]. With [[accuracy]], we basically ask what percentage of observations your model assigned to the correct class. This is totally worthless when the classes are unbalanced. 

We can create a *confusion matrix*. This shows the percentages that fall into each class, and the accuracies with which we were able to classify things inside each class.

|          | Predicted A | Predicted B |
| -------- | ----------- | ----------- |
| Actual A | %           | %           |
| Actual B | %           | %           | 

Using this matrix, we can see the rate of false positives, false negatives, true positives, and true negatives. This can help us see where our model is doing well, and where we need to do some work. These matrices can also be useful in multiclassing problems.

[[classification]] models predict class, and can return a probability of the observation falling into that class. We can use [[Scikit-Learn]]'s `predict_proba` to get the predicted probability rather than just the final class assignment. By default, classification models use a *classification threshold* of 0.5. This means that if there is a 51% chance of an observation being part of class A, into class A it goes.

However, we can change this classification threshold. If we increase it, we're making fewer observations end up being put into class A, because the model has to be more than 50% confident that the observation is really in that class. 

We have to consider the tradeoff between true positives and false negatives. By increasing the threshold, we shrink the bin for class A and widen class B's bin. This will increase our rate of false Bs and decrease our rate of false As. For certain use cases, this is great, but for others it would be a disaster. For example, a classification model looking for tumors should care a lot more about flagging anything that could possibly be a tumor, rather than caring about preventing false postives at all costs. 

To visualize this, we can use something called a *Receiver Operating Characteristic curve*, or ROC curve. We plot the false positive rate on the x-axis, and plot the true postive rate on the y-axis. Note that we've shifted terminology somewhat, and are now referring to class A as positive, and class B as negative. [[ROC curve plots false positives vs true positives]]. 

![roc curve|500](https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png)

We judge the performance of a classification model by the area under this curve. Ideally, the area would be 1.00, representing a model that never gives us false positives or false negatives. In the chart above, the NetChop model is doing the best (in red). At the optimal cutoff point, the model is classifying about 60% of the truly positive values as positive, while about 30% of the positives are, in reality, negatives.

There are other metrics that we should know about: *precision and recall.* [[precision]] asks, "of those predicted to be positive, how many actually are positive?" [[recall]] asks, "of those actually positive, how many were predicted to be positive?"

When getting one class right is more important than the other, as was the case for the hypothetical tumor detector, we care about the recall of the model more than the precision. 

Mathematically, precision is defined as the true positives as a proportion of all predicted postives. Recall is defined as the proportion of predicted positives out of the true positive observations. 

We can plot the relationship between recall and precision.

![a|500](https://machinelearningblogcom.files.wordpress.com/2018/04/bildschirmfoto-2018-04-03-um-11-31-16.png?w=1024)

We have a heuristic for balancing precision and recall, called the *F1 score.* The F1 score represents a balanced approach that takes precision and recall into account equally.

$$ F_1 = 2*\frac{precision*recall}{precision+recall} $$

We can adjust the importance of precision and recall using a modification of this formula, called $F_\beta$. 

Sometimes we might want to see how close our model came to being wrong about particular observations. If we want that information, we need to look at *log loss cross-entropy.* 

$$ - \sum_{i=1}^{N} y \cdot log(p_{y}) + (1-y) \cdot log((1-p_{y}))$$

- $p_y$ = predicted class
- $y$ = actual class

This equation penalizes the model for being close to making a mistake. A lower value for log-loss is better. 

There are lots of metrics, so it's important to know which ones to use for your situation. First, we should use a ROC curve to choose a model. We can then make a confusion threshold to tune our classification threshold. If the classes are balanced, we could use accuracy, but otherwise we should use precision and recall to further tune the threshold. 

For a code implementation, see `classification_error_metrics_student.ipynb` in my repository.
	
## Career workshop - [[networking]]
- Lots of people hate networking, especially in tech since people tend to be introverts
- Fun facts
	- 85% of jobs are found through networking.
	- 50% of jobs are unadvertised.
	- 72% say that appearances impact their impression of someone. 
- Step 1: start with a positive attitude
	- View networking as an opportunity to meet similar people. 
	- Networking is *ongoing relationship-building*, not transactional, one-off conversations. 
	- Even someone who doesn't get you a job becomes one of your permanent contacts for the future. 
- Step 2: maintain the right perspective
	- We network to *expand our network*, not to straight-up get a job. 
	- Have an outlook of resilience and grit. We won't get an offer right away. 
	- We won't be *given a job*, even if we get a referral. We have to get a job ourself. 
	- When we reach out, we want to be direct in what we want out of the conversation. 
	- There are 800+ alumni from Metis. Reach out to them!
	- Don't worry about bothering them. They've all been right where you are. 
- Step 3: get into action and meet people
	- Find meetups and events that look interesting.
	- Strike up conversations with at least 5 people at each event.
	- With online events, there might not be an open networking event, but you have to just make it happen. If there is only a Q and A, ask a question and use the opportunity to introduce yourself. Make sure to get people's info. 
	- Engage at least twice a week, either through events or by talking to people on forums.
- Step 4: add value wherever possible
	- Provide an introduction about yourself.
	- Share things that you've been reading or thinking about related to the field. 
	- Provide insight or ask questions about a domain that you're both interested in.
- But what do we actually say?
	- Ask open ended questions and let others do the talking.
	- Use active listening to develop pointed questions.
	- Share what you're doing and what you're looking for.
	- Create next steps in the conversation - exchanging contact info, etc. 
- Maintain perspective
	- Not everyone will be helpful, interesting, or nice
	- Have high hopes for the process and low expectations of individuals
- Writing emails about a position
	- Create email signature with contact information!!! Links to all socials, github, etc
	- Don't come on too strong out of the gate with a request that will take lots of work. 
	- Do your research about the person you're emailing.
	- Include a bio about yourself and include links to your resume and the position you're interested in. 
	- Follow up and be persistent. 
- Messaging a new contact
	- Follow up with a reminder of who you are
	- Have a purpose with the follow up
	- Be specific 
	- Ask some good questions that show you remember them. 
- The bootcamp factor
	- We have to know how to describe Metis. 
	- Some people have no idea what a bootcamp teaches you, while others are biased.
	- Paint a picture of the kinds of projects that you made, empasize that *you* did this, not that the bootcamp made you do it. 

## Lecture - [[postgres]]
- See my notebook `00_pandas_and_postgres.ipynb`
- Overview
	- We will be working with the names database
	- We can use [[SQL]] functionality built into [[Pandas]]
	- Or we can use a cursor based method
- [[Pandas]] and `read_sql`
	- We need to import `pandas.io.sql as pd_sql`
	- We can write a query as a string, and then use `pd_sql.read_sql(query, connection)` to pull from a [[SQL]] [[database]]
- Cursor method
	- If the dataset is really big, we can't just pull it into a df.
	- Cursor looks at one record at a time and fetches it one-by-one
	- We have to create a cursor connection, and then we execute our query using the cursor, and then 'fetch' rows of data. We can use `cursor.fetchone()` to get one record, `cursor.fetchmany(n)` to get *n* records, and `cursor.fetchall()` to fetch the rest. 
	- To do anything, we just use `cursor.execute(query)`. If we wanted to change the region of RI in the database...
	
```python
query_fix_RI = """
UPDATE region SET region='New_England' WHERE state='RI'
"""

cursor.execute(query_fix_RI)
```

-        
	- This will change it locally, but the change won't be global yet. 
- Commits and rollbacks
	- When we execute a change to the database using postgres, we have to ==commit== the changes similar to what we would have to do in git. 
	- In order to commit the change from above, we have to run `cursor.execute('commit;')`. After we do that, the change we made became global for the database, wherever we access it from. 
	- If we want to, we can change a parameter when we set up our connection, so that `connection.autocommit = True`. But be careful if you do that, for obvious reasons. 
	- After doing a commit, we have to refresh our cursor by running `cursor.execute('BEGIN;')`. If we don't do this, it will just keep auto-commiting everything after the initial commit command. 
	- If we make a mistake and want to undo it, we can initiate a ==rollback==. We type `cursor.execute('rollback;')`. 
- Repairing typos
	- PostgreSQL cursors are *really dumb* when it comes to typos. 
	- If we make a typo, we can't simply change the command and move on.
	- We have to actually do a rollback! How dumb is that?
- When to actually use a cursor
	- It makes the most sense to use a cursor when you're short on memory. If you want to avoid loading in the whole dataset it's useful. 
	- But we could also use the `LIMIT` command in `pd_sql`. 