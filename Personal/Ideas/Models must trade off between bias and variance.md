*Models must trade off between bias and variance.* Basically, [[machine learning]] algorithms must fall somewhere on the [[bias-variance tradeoff]]. 

As we seek to decrease the bias (the inability of a model to grasp existing patterns in the dataset), we risk increasing the variance (the degree to which a model's prediction fluctuates in between runs).  

For example, [[linear regression]] is pretty simple, and tends towards bias. On the other hand, [[deep learning]] models are really complex, and can end up with a ton of variance. 

There are ways to combat both over and underfitting. 
- [[How can we combat overfitting?]]
- [[How can we combat underfitting?]]

#idea/compsci/data-science 

---
