---
aliases: [attention mechanism]
---

# Attention Mechanisms


---
My understanding of these things is still very shaky, but I believe fundamentally an attention mechanism is a way to determine how much weight a [[deep learning]] model, such as a [[neural network]], should be applying to each word in a sequence when trying to decide what the next word ought to be. 

The [[transformer]] architecture is a neural network centered around attention mechanisms entirely, as outlined in [[Attention is All You Need (2017)]]. 