# How do transformers work?
I know that a [[transformer]] is a very powerful [[neural network]] architecture. 

Wikipedia [1] says that a transformer is a deep learning model that "adopts the mechanism of self-attention."  [[What is attention in machine learning?]] The article also says that transformers allow for more parallelization than a [[recurrent neural network]] due to the fact that RNNs must process input data sequentially. [[Parallelization speeds up computation]], thus making transformers faster than RNNs. 

Moving on, the wikipedia article says that [[Transformers use an encoder-decoder architecture]]. 

---
#question/data-science 

1. https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)