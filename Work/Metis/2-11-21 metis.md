# Metis Notes for Thursday, Feb 11th
#meeting/career

---
Today is the fourth day of [[metis week 6]]. We're starting off with a pair programming exercise, and then we have lectures on [[unsupervised learning]] and natural language processing. Later in the day, we will be introduced to our fourth project. 

## Pair exercise
See `Pair_naiveBayesStarter.ipynb`. 

[[Naive Bayes can measure sentiment]]. The model can just count up the occurences of all the words in the *corpus*.

| Statement         | Review   |
| ----------------- | -------- |
| I love this movie | Positive |
| This song is good | Positive |
| I hate this movie | Negative | 

So the model would say that "hate" has a higher [[probability]] of appearing in a negative review than the word "love" does. 

Caveats: naive bayes has to be given at least a little probability of any word appearing in each review. This is because we don't want to end up multiplying the probabilities by zero if a never-before-seen word appears in a review. 

We can create default [[dictionaries]] to track how frequently words appear in positive vs negative reviews.

```python
pos_words = defaultdict()
neg_words = defaultdict()

for i in length(pos_reviews):
	for word in pos_reviews[i]:
		pos_words[word] += 1
		
for i in length(neg_reviews):
	for word in neg_reviews[i]:
		neg_words[word] += 1
```

Now we wnat to create probabilities that each word would appear in the positive vs negative reviews.

```python
pos_probs = defaultdict()
neg_probs = defaultdict()

for i in range(len(pos_words)):
        for word in pos_words[i]:
            pos_probs[word] = pos_wordcounts[word] / (pos_wordcounts[word] + neg_wordcounts[word])
            
for i in range(len(neg_words)):
    for word in neg_words[i]:
        neg_probs[word] = neg_wordcounts[word] / (pos_wordcounts[word] + neg_wordcounts[word])			

```

But we have to include words that are only in one or the other, otherwise we would have 100% chance of positive if we encountered a word that was only found in the positive reviews. 

```python
for word in pos_probs:
    if word not in neg_probs:
        neg_probs[word] = 0.001
        pos_probs[word] = 0.999
        
for word in neg_probs:
    if word not in pos_probs:
        pos_probs[word] = 0.001
        neg_probs[word] = 0.999
```

Now we convert to log probabilities.

```python
for word in pos_probs:
    pos_probs[word] = np.log(pos_probs[word])
    
for word in neg_probs:
    neg_probs[word] = np.log(neg_probs[word])
```

Now we can use these dictionaries to sum the probabilities of each word in a new review, and say what probability the review has for being positive or negative. 

```python
def predict(text):
	text = text.lower()
	positive_prob = 1
	negative_prob = 1
	for word in text.split():
		negative_prob += pos_probs[word]
		positive_prob += neg_probs[word]
		
	print('probability of positive:', round(positive_prob,3))
    print('probability of negative:', round(negative_prob,3))
```

## Shortcut using CountVectorizer
We can use sklearn to accelerate this process.

```python
from sklearn.feature_extraction.text import CountVectorizer
```

CountVectorizer takes a list of [[strings]], and separates each string into unique words and count them up. We can also add an `ngram_range` that takes a tuple and creates a sliding window if we want to see how often pairs of words exist side by side. We can increase the size of the ngram to include larger and larger groups of neighboring words. Note that this will increase the number of features by *a lot*. 

If we want to prevent an overflow due to too many features, we can set a maximum feature parameter. This will only keep the top *n* ngrams and words based on their frequency in the corpus. 

Sklearn stores this matrix as a *sparse matrix* that includes only the cells that have a non-zero value. This helps cut down on memory needs. Don't take it out of this format unless you have to, since sklearn will work great with this format. 

After we extract the features from the corpus, we just assign labels and now we can use other sklearn models to try and classify the text. 

## When to use certain metrics
ROC AUC is one level abstracted. We haven't commited to a cutoff yet so we can see the results of all possible cutoffs. 

When we want to actually select a threshold, we have to move beyond AUC

## Lecture - [[NLP]]
[[NLP]] stands for *natural language processing.* The first step is usually to use CountVectorizer for preprocessing. That's pretty standard. Then if we have labels we can just put it straight into a [[classification]] model. If we don't have labels, then there are a few more steps. 

The issue with straight vectorization is that we end up with *a ton* of features. Too many features is a problem because it tends to lead to overfitting. We would like to reduce the number of features without reducing our access to the information present in the [[data]]. 

This means we want to engage in [[dimension reduction]] in some way. We could look at colinearity and try to get rid of superfluous features. We could also get rid of words that aren't useful for our specific use-case. For example, if we were measuring sentiment, we could get rid of all words that have no sentiment. We could also use *stemming,* where we combine similar words during vectorization. For example, we would combine "walk," "walks," and "walking." After we vectorize, we could also just perform mathematical [[dimension reduction]].
 
If we are classifying, there's really no need to reduce the dimensionality. But if we are [[clustering]], this process is critical.

There are various NLP toolkits available. We can use NLTK, which stands for natural langauge toolkit. This can do lots of cool stuff. 

The formal word for extracting features from text is *tokenization.* We can tokenize into words, n-grams (combinations of n-words), or sentences. We could also tokenize using [[regex]], which could be helpful for a specific use-case. 

After we tokenize, we want to apply some sort of stemming procedure to the *corpus* (the collection of documents). There are lots of different stemming protocols in the NLTK package. We don't have to do this right away. Stemming does decrease interpretability a little bit though. We can actually think of stemming and lemmatization as a form of [[regularization]]. This is because we are engaging in [[dimension reduction]] when we stem or lemmatize. 

If we're interested in a specific part of speech, such as nouns or adjectives, we can perform *parts of speech tagging.* NLTK has a tool for this, and then we can filter by part of speech based on our use case.

Now we can get a bit fancier. We can actually perform *named entity recognition* to attempt and extract actual subjects from the text. For example, a process like this would turn `['united','staate']` into `['united_states']`. 

*Compound term extraction* is the process of telling our model about multi-word expressions. We could use terms generated through named entity recognition, or we could use our domain knowledge.

### Comparing text
When we compare texts, we can look on the word level or the document level. 

If we want to look at word similarity, we can start with a simple approach. we can see how similar words are on a character-by-character basis. This approach would say that BATH is similar to MATH. In order to do this, we can use *Levenshtein distance*. It calculates the minimum number of operations to get from one word to another. This can be useful for spellcheck, but modern spellcheck leverages far more powerful tools. 

If we want to talk about document similarity, we need to use a metric to determine the 'distance' between documents. This could be [[euclidian distance]], cosine distance, or jacard distance. 