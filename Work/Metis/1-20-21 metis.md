# Notes for Wednesday, Jan 20th
`LINKS:` [[metis week 3]]
#meeting/career

---
It's now the middle of [[metis week 3]]. We are starting with a pair practice on [[linear regression]] again, and then we have a lecture on stochastic gradient descent, followed by a session going over answers to the linerar regerssion assignment, and then in the afternoon I have a 1-on-1 meeting with Brian, one of the instructors to talk about [[natural amenity regression]]. 

# Lecture: linear regression practice
- Before we get into that, let's talk about grades
	- Metis doesn't really like the idea of grades but they have to because of the accrediation process. 
	- Every category starts at a 3, and then if you go above and beyond the requirements, you might get a bump for that category. 
	- This numerical feedback is paired with verbal feedback and you shouldn't go crazy if you aren't getting above three all the time. 
- Colinearity
	- Our model can be terrible if there's two colinear features.
	- [[regularization]] avoids this by reducing the weight of features that are colinear. [[ridge regression]] will give one of the features a tiny coefficient, and a[[LASSO regression]] will just erase one of them outright. 

# Lecture: [[gradient descent]]
Fundamentally, gradient descent is a method of *model optimization.* There are lots of ways we can perform optimization, but gradient descent is very generalizeable and is a great place to start. For a code implementation, see my `gradient_descent.ipynb` notebook.



- Overview
	- This is all about optimization
	- There are lots of ways to do optimization. Gradient descent is very generalizeable and is a good place to start
	- See my notebook, `gradient_descent.ipynb`
- Our goal
	- We want to find a point that minimizes the cost [[function]] of our model. 
- Tangent lines and steepest descent
	- Whenever we create a linreg called `lm` and call `lm.fit()`, we are already doing gradient descent.
	- When we take the [[derivatives|derivative]] of a function at point $x$, we find the instantaneous rate of change of the function at $x$. For example, since the derivative of $x^2$ is $2x$, we would get an instantaneous rate of change of 6 if we plugged in a value of 3. This means that the slope of the tangent line touching the curve of $x^2$ at the point 3 is equal to 6. 
	- As the instantaneous slope decreases, we get closer and closer to local [[max and min]]s. The instantaneous slope will be equal to zero at this point. 
## Gradient descent: first pass
- Imagine we start at a random point on the curve and want to find the minimum. 
- All we need is the ability to find the derivative over and over. 
- The ==update rule== can help us out with this. For each step, update the current x coordinate $x_n$, via...
	 $$x_{n+1} = x_n - \alpha\frac{df}{dx}(x_n)$$ 
- ...where $\alpha$ is a constant called the ==learning rate==. 
- We calculate the derivative at point $x_n$. and then subtract the x-value at $x_n$ by this derivative.
- EX: For the function $x^2$, if we started at the point x=10, the next point we check is going to be 10-20, which is x=-10. So we didn't get anywhere. This is why we need the ==learning rate==. This rate helps us make sure we don't fly by the local minimum. This is a hyperparameter that we usually set as a small number, like 0.1 or something. 
- If we use a learning rate of 0.5 for the above example, we would evaluate x=10, then we would evaluate $x=10-0.5*20$, to end right at the local minimum. Of course, we probably wouldn't have the perfect learning rate, but we would get there eventually with small steps. 
- We can create a function to do this...

```python
alpha = .2 # learning rate
tol = .0001 # tolerance level for stopping criterion
x = np.random.uniform(-6,6) # random starting point
        
display.clear_output()
for _ in range(100):    
    # simple animated plotting
    plot_tangent_at_pt(f, df_dx, x, np.linspace(-6,6))    
    display.clear_output(wait=True)
    time.sleep(.5)
    
    # gradient descent update rule
    x_delta = alpha * df_dx(x) 
    x = x - x_delta
    
    # check stopping criterion
    if np.abs(x_delta) < tol:
        break  
```

- Code explanation
	- This function has a learning rate of 0.2, and a random starting point between -2 and 2 on the curve of $x^2$. We can see that it steps down the curve one step at a time. 
- Choosing a learning rate
	- When we choose a learning rate too high, the function just skips right over the local minimum and the process repeats forever. 
	- If the learning rate is too small, it will approach the local minimum very slowly. 
- Generalizing gradient descent to many dimensions
	- We can't really visualize this, but the equation still works for any number of dimensions. 
	- For a function with *n* variables, the gradient is a vector valued function defined as... $$ \nabla f = (\frac{\partial f}{\partial x_1}+\frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}) $$
	- ... where $\partial$ indicates that this is just a partial derivative of $f(x)$.
	- Example: $$f(x,y) = x^2 + 3 - y$$ $$\nabla f(x,y) = \frac{\partial }{\partial x}(x^2 + 3 - y)$$ $$\frac{\partial }{\partial y}(x^2 + 3 - y) = (2x, -1)$$$$\nabla f(3,5) = (6, -1)$$
	- Even with two variables, it's still sort of straightforward. But what if we have 1,000 or more variables?
	- We need to generalize this more. We can rewrite the ==update rule== again...
	$$\mathbf{x_{n+1}} = \mathbf{x_n} - \alpha\nabla f(\mathbf{x_n})$$
	- Note that these bold characters above indicate we're talking about vectors instead of single numbers.
- Plotting in 3D

```python
def plot_gradient_2D_at_pt(f, f_grad, p, xy_range, step=None):
    """
    Plot a surface and the direction of steepest descent
    (negative gradient vector) at the point (p,f(p)) on the surface
    
    Parameters:
    -----------
    f: function describing surface to plot
    f_grad: gradient of f
    p: x, y coordinates to calculate gradient at
    xy_range: x and y axis values to plot, should be symmetric about 0    
    """
        
    # compute f(x,y) and gradient
    x, y = p[0], p[1]
    z = f(x, y)
    grad_x, grad_y = f_grad(x, y) 

    # create grids for 3D plotting
    X, Y = np.meshgrid(xy_range, xy_range)
    Z = f(X, Y)

    # create plot with a 3D projection
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    
    # plot surface and highlight p, f(p) on the surface
    ax.plot_surface(X, Y, Z, cmap='viridis_r', alpha=.5)
    ax.scatter(x,y,z,c='r',s=70, marker='*')
    
    # draw line showing negative gradient vector 
    ax.plot([x, x - grad_x], [y, y - grad_y],
            [z, f(x - grad_x, y - grad_y)])

    title = ('Surface f(x,y) with negative gradient vector at ' 
              '({:.2f},{:.2f},{:.2f})'.format(x,y,z))
    if step is not None:
        title += f"\n Step: {step}"
    plt.title(title)
    plt.show()
	
def f(x, y): 
    return (x**2-8*x+7)/3 + (y**2-5*x+8)/6

# check that your math gave you this answer
def f_grad(x, y):
    return (2*x - 8)/3 - 5/6, y/3 

plot_gradient_2D_at_pt(f, f_grad, (-8,5), np.linspace(-10,10,200))
```

- Code explanation
	- This is just for teaching purposes
	- This will create a 3D plot showing the line of steepest descent of this function with 2 variables. 
- Applying gradient descent to the above situation

```python
alpha = 1 # learning rate
tol = .001 # tolerance level for stopping criterion
x, y = np.random.uniform(-10,10, size=2) # random starting point
        
display.clear_output()
for i in range(100):    
    # simple animated plotting
    plot_gradient_2D_at_pt(f, f_grad, (x,y), np.linspace(-10,10,200), step=i)
    display.clear_output(wait=True)
#    time.sleep(.5) 
    
    # gradient descent update rule
    delta = alpha * np.array(f_grad(x,y))
    x, y = x - delta[0], y - delta[1]
    
    # check stopping criterion
    if np.linalg.norm(delta) < tol:
        break  
```

- Code explanation
	- We can see that this is a version of the earlier gradient descent method, but applied to three dimensions. 
## Applying gradient descent to linear regression
- Gradient descent is very useful when we're in situations where we can express the quality of fit as a function of the model parameters. 
- We just have to think of our LinReg as the function... $$ y_{pred} = \beta_0+\beta_1x_1+...+\beta_px_p$$
- Where we have *p* variables. 
- Now we just have to minimize the following [[loss function]]... $$ J(\beta_0, ..., \beta_p) = \frac{1}{2}\sum_{i=1}^{n} ((\beta_0 + \beta_1 x_1 + ... + \beta_p x_p) - y_i)^2 $$
- ... in order to achieve the best fit by varying all the coefficients. 
- We can use the sum rule, the chain rule, and other things that I don't understand to arrive at the final function... $$\frac{\partial J}{\partial \beta_j} = \sum_{i=1}^{n} ((\beta_0 + \beta_1 x_1 + ... + \beta_p x_p) - y_i) x_i $$
- ==All linear regresion cost functions are convex==, so we don't have to worry about accidentally finding the local maximum. 
- What if there are multiple local minima?
	- That's where this way of doing things might not work. Depending on where we started, we might get stuck in one of the local minima that isn't the smallest one. 

## *Why* gradient descent?
- This is a pretty complex situation, and gradient descent is a good way of minimizing that complexity. 
- Enter *stochastic* gradient descent
	- We don't need to know the exact gradient to know which direction to head in. 
	- We can form mini batches of a fixed size, and use these to estimate the gradient at a certain point. This approximates the gradient so we know which step to head in. 
	- This is faster than calculating a gradient for every single step. 
	- No matter how large the dataset is, the time complexity is just dependent on the size of the mini batch that we calculate with. This is great! No matter how many features we have, and no matter how many observations, our calculations of the local minimum won't take an insane amount of time. 
	- Theoretically, the time complexity approaches $O(in_bm)$, where *i* is the number of iterations, $n_b$ is the batch size, and $m$ is the number of features.  
	- It's also nice to introduce some fuzziness, because it helps the model generalize. It prevents the model from exactly optimizing itself for the training set. In this way, we can think of stochastic gradient descent as a form of [[regularization]]. 

## What about non-convex functions?
- We can escape the trap of falling into non-absolute minima by restarting the SGD at various points and seeing which one results in the lowest error. 
- Or, we could give the "ball rolling down a hill" some momentum, so that it would hit the local min, and then roll a little bit up the hill on the other side. If it doesn't find another hill up there, it will roll down again back to the minimum. 

# Questions for my meeting with Brian
- Why do we sometimes standard-scale our [[data]] at the beginning, but in some examples we're standard-scaling the data each time we fit a regression?
- What are some ways I can regularize my model even further, to improve my r2?
- What is considered a good r2 for a house price predictor?
- What audience should I have in mind for my blog? What about my presentation?
	- Presentation is for an imagined audience that you're pitching your project to. 
	- Blog is more directed towards others in the industry. 