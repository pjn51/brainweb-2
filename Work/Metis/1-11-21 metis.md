# Metis Notes for Monday, 1-11-21
`LINKS:` [[metis week 2]]
#meeting/career

---
Today will start with a pair programming exercise where we will get an intro to HTML, and then we have a lecture on [[web scraping]] and [[machine learning]], with a final intro to [[natural amenity regression]]. 

## Pair programming: [[HTML]]
We need to be familiar with HTML in order to use web scraping packages. The assignment was to create a very simple html page using tags and stuff.

```html
<html>
	<body>
		<h1>This is a Heading</h1>
		<p>This is a paragraph.</p>

		<div id= 'linkdiv'>
		  <a href>https://www.google.com</a>
		</div>

		<div id='imgdiv'>
			<img src="replace with url of your image" alt="the hovering text"/>
		</div>

	</body>
</html>
```

Our project involves linear regression and web scraping. We will be brainstorming with a couple others what would be interesting projects and where we could get the [[data]] from. We will be assigned new project buddies based on the similarities of our projects. 

# Lecture - [[web scraping]]
For a code introduction, see `web_scraping_beautifulsoup.ipynb`. 

When we want to get information from the web, we need to scrape that information. To scrape it, we have to understand [[HTML]]. Every HTML file has a tag indicating a start to the HTML, `<html>` and a tag indicating the end of the HTML, `</html>`. There are two sections to the HTML, the head and the body. Inside the head, we import stuff and have the name of the site. In the body goes everything else. 

We want a program to grab info from a website so that we can use it as [[data]]. For that program, we can use `requests`. All `requests` does is scrape all the HTML from a site. We need a *parser* to read the HTML and allow us to extract the important bits. 

For the parser, we can use BeautifulSoup. First, we have to import both of these tools.

```python
from bs4 import BeautifulSoup
import requests
```

We can use *tags* to locate things within the HTML. If we just want a line of text, we know the tag name will correspond to the paragraphs of the website, and we just have to find what kind of tag the author of the site is using to refer to paragraphs. 

```html
<tag-name attr1="value of attr1" attr2="value of attr2" .... attrN="value of attrN">
    Inner text of the tag
</tag-name>
```

We should also be familiar with [[OOP|py classes]] and IDs. Classes are non-unique signifiers, and IDs are unique to each object. 

We can display HTML in [[Jupyter Notebook]]. In order to do this, we have to pass `from IPython.core.display import display, HTML`. This allows us to execute `display(HTML(my_html))` and look at our HTML code (`my_html`) in the notebook.

When we want to start the scrape, first we pass a URL to `requests` and then we create a HTML object using BeautifulSoup. We can use `response.status_code` to see if this worked. We want it to respond '200', not '404'.

```python
from bs4 import BeautifulSoup
import requests

url='https://google.com/'

html = requests.get(url)

text = html.text()

soup = BeautifulSoup(html, 'html5lib')

```

Now we can try and find a specific tag from within the soup object, using `soup.find()`. This will return a bs4 element. We can look for a single tag, or we can get all the tags using `soup.find_all()`. This will return a list of all the elements with that tag.

```python
soup.find('div').text

soup.find_all('span')

```

We can be even more specific. Some elements on the page may have the same tag, so we want to narrow our search down. We can pass `soup.find_all('div', class_ = 'article')` to find all div items that have an 'article' class. 

As for how we actually figure out what search criteria to use, we can simply use the inspect tool in Google Chrome. 

If we want to get links, we can use `.get()`.

## Web scraping pipeline
What if we want lots of information from a website, where everything is coming from a different location? We would have to find alot of class info, and then clean up the output in python through some classic tricks for strings. 

For example, if we want a `<span>` that has no class info, and if that `<span>` is inside a `<div>` that has no class info, we could just look for text within the whole HTML document. If we know we're looking for "abc," we could pass `soup.find(text='abc')` and pull all the instances of "abc." Beware that this is an exact match.

If we don't want an exact match, we have to use [[regex]]. 

We can also pull the next element *after* an element that we know the location of. If we locate a BeautifulSoup object and call it `obj`, we can pass `obj.findNext()` to find the next object in the HTML document. 

Unfortunately, web scraping isn't perfectly generalizeable. Every webpage has a different format and stype for organizing elements, but hopefully we can use the same format for multiple pages in a site. 

There's also the issue of websites that don't like our presence. Many websites are totally ok with web scraping, but others try and prevent us. If they detect any indication that we're scraping, they will block our IP address. We can add time in between requests to blend in with the normal users. 

With dynamic sites like social media, we have to use *selenium* instead because we can't simply download the HTML once and be done. 

# Lecture - [[machine learning]]
Machine learning has lots of terms that are used interchangeably. What is machine learning really? It's a field of artificial intelligence in some ways. Therefore, we have to have a brief context of what the field of AI looks like today.

In the 1980s, people defined AI as a system that allows a machine to learn. The idea of a [[neural network]] was deveoped in an attempt to mimic human brains. Researchers built vision interfaces, tactile sensors, and other things to try and approach artificial intelligence by mimicing humans. 

However, mimicing nature is not always best. For example, our airplanes are nothing like birds. Researchers decided to start simpler rather than try to replicate the way our brains work. Researchers turned towards the weaknesses in human intelligence, attempting to use AI to do those kinds of tasks. This was much more successful. Machines tend to be good at what we're bad at, and vice versa.

Humans are bad at handling lots of information at once and making accurate predictions about the future. Researchers tried to build tools that could handle these sorts of tasks. 

The basis for all of these tasks was access to *data.* We take this access for granted today, but it has been a huge historical limitation. The first method of collecting data was the survey. The census broadened our horizons, moving from a sample to understanding a whole population (see [[sample and population]]). However, we still had limitations. The human genome project was another landmark in the quest for information. This opened up the field of machine learning, as finally there were tasks that we had tons of data about. The arrival of the internet further scaled up the potential for [[data]] analysis. Today, [[We've entered a new age of data availability]]. 

## Kinds of ML
Now we can turn directly to machine learning. ML is a branch of AI that deals with the construction and study of systems that can learn from data. We deal with *representation*, the extracting of structure from data, and *generalization*, the making of predictions from data. We aren't in the business of memorization. 

There are two kinds of machine learning. We have [[supervised learning]], which is when we have data points with known outputs. This means we are learning from labeled data, where we know the 'correct' answer right off the bat. Handwriting analysis is an example of this, because we know that an observation is a 'w' before we analyze it. The real trick here is just to get the computer to be able to take the correct path from input to output. Another classic example is an email spam filter.

We also have [[unsupervised learning]], which is a little trickier. With these sorts of tasks, we have no known target. It's more open ended and there's less we can easily measure. For example, we might have a housing dataset with no known prices. We could still use [[clustering]] to see which groups emerge, but we won't be able to automatically label them. 

Usually we have two steps in [[unsupervised learning]]. First, we try and find patterns, and then we have to investigate these patterns and see what we can learn. 

There is actually a third kind of learning, called *reinforcement learning.* We won't be covering this as much.

[[ML approaches are question-specific]]. 

## Kinds of Data
Targets can be *continuous* or *categorical.* If the target is continuous, we use [[regression]]. If the target is categorical, we use [[classification]]. 

For unsupervised learning, a continuous target means we can use *[[dimension reduction]].* A categorical variable can lead us to use [[clustering]]. 

We can think of data as a vector space. This is the n-dimensional space where the data exists graphically. Each point in space is called a *record* or an *observation.* The n-dimensionality is determined by the number of features per observation (*n*). 

## Feature Engineering
Feature engineering is the art of using features in unique ways to find patterns. Maybe our model isn't working so well when we use $y = mx_1 + mx_2$, so we try $y = mx_1 \cdot mx_2$ instead. There are many ways that we can recombine features. 

We can think of [[data science]] as the process of [[function]] approximation, feature engineering, and finding structure in the data. 

Just because a model is complex doesn't mean that it is inherently good. Complexity shouldn't be used to show off. We should remember [[Occam's Razor]] and focus on *solving problems,* not just using every tool in the toolbox. [[Complexity must be justified]]. 

# [[natural amenity regression]] introduction
We will be doing supervised learning, and incorporating a regression analysis. This means the target must have a numerical continuous value that is known to us. A significant portion of the data must be scraped from the web. 

An easy one would be box office data through the box office mojo site.

Considerations
- We need a few hundred rows of data, preferably 1000. Remember that you're gonna end up with less data than you started with so get a bunch. 
- We also want a lot of features. Ten would be a good benchmark. 

