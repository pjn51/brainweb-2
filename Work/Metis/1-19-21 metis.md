# Notes for Tuesday, Jan 19th
#meeting/career

---
Today is the first day of [[metis week 3]]. We are starting with a pair programming exercise practicing linear regression, with a discussion afterward. We also have a lecture going over the LinReg assessment we took last week, and one on the assumptions that linear regressions make.  

# Lecture: chatting about our [[linear regression]] pair problems
- What our attitude should be
	- Whenever we are going for an interpretive or a predictive model, we want the model to be predictive. We want to minimize the error when tested against new [[data]]. 
	- While this is a bootcamp and it is intense, think of it also as a music festival. You cannot go to every band that you want to. In that way, it's going to be impossible to fully dive into every topic that we cover here. 
	- It's better to build iteravely and flexibly, rather than trying to incorporate every tool and method that you possibly could. 
- Before we look into the problems, let's think about training scores vs test scores
	- A ==low training score== and a ==low testing score==means that the model is underfitting the data. Could be too simple, not the right features, or random target.
	- A ==high training score== and a ==high testing score==means  good fit. We're happy with this. 
	- A ==high training score== and a ==low testing score== means the model is overfitting the training data and isn't generalizing. Too complex, or not enough data.
	- ==Low training score, high testing score==: potential outliers in the training set. We could use cross-validation to get at this issue. 
- Transforming our models
	- What if we had a model that got a pretty good r2, but we want to improve it. Then we added a new feature that was 2 times another feature. The score would actually stay the same because no new information was added. 
	- In general, ==linear transformations don't change model scores==. 
	- If we want to actually change our model, we need to do something non-linear, like squaring one of the features. If we do this, keep the original one in the regression. 
	- What does scaling the features really do?
		- It only makes a difference when we're using [[regularization]]. Otherwise, it's just for our own ability to interpret what the coefficients really mean. 
- What does r2 even mean?
	- In some applications, a score of 0.4 is record-breaking, while in others a score of 0.95 is a total joke. 
	- It's more important to say what your model can actually predict rather than just using the score on its own. 

# Lecture: Linear Regression Assumptions
For a code walkthrough, see the `linear-assumptions-testing.ipynb` notebook.

What are we assuming about the data when we use a linear regression? We have to remember the golem analogy. These models are smart, but they aren't wise. They'll just do whatever we tell them to.

## Assumption 1: linearity
*We assume the relationships in the data are linear.* Most relationships aren't really linear, but they just have to be sort of linear within the specifications that we're trying to predict. A linear model of height vs age works fine between the ages 5 and 20, but it would predict old people to be giants. See the issue?

This is a linear relationship: $Y= {\beta_0}+ {\beta_1}x_1+{\beta_2}x_2 +{\epsilon}$

This is a non-linear relationship: $Y= {\beta_0}+ e^{\beta_1}x^{\beta_2}$

The lines in a plot of the variables may be curved and still be linear, we're talking about if the coefficients are scalars or not. The second equation above doesn't have scalars for coefficients, and therefore they change x in multiple ways depending on where you are on the x-axis.

In order to determine linearity, we can plot the [[residuals]] of our linear regression and use a *normal Q-Q plot.* This lets us see if the relationship is linear enough. The normal Q-Q plot should ideally have points following a straight diagonal line if the relationship is perfectly linear.

Q-Q plots are basically histograms of the residuals. On the y-axis, we see the residuals ordered from smallest to largest. 

 ![common Q-Q deviatons](https://i.stack.imgur.com/ZXRkL.png)
 
 The above image shows some common issues in Q-Q plots. Seeing something like this indicates that there are non-linear relationships at work. If this happens, we need to move beyond a linear regression to model the patterns.

## Assumption 2: normally distributed error
The variance has to be generally consistent as we move along the x-axis in the data space. This can be violated when the target feature isn't a continuous variable. For example, if we're counting items then a standard linear regression wouldn't work so well. 

## Assumption 3: constant variance of error term
This assumption is violated if the residuals grow larger and larger as our predictive features increase in scale. This indicates *heteroskedasticity.* "Hetero" means different, "skedasis" means dispersion.

![heteroskedasticity|400](https://www.investopedia.com/thmb/683k04t0ZABfer3Vq-TrQgi9WTQ=/1787x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/Heteroskedasticity22-ce5acc2acef6494d91935588b0599579.png)

We can counter this by using transformations to make the data easier to work with. For instance, the *box-cox transformation* can be found in the [[Scikit-Learn]] package, and we seek to find an optimal $\lambda$ value to normalize the residuals. We aren't trying to normalize the *features,* just the *resudials.*

## Assumption 4: errors are uncorrelated across observations
This is a common issue when working with time series. This assumption is violated when there's a strong relationship between residuals and time. This would mean that the model isn't taking the time feature into account. For example, we might be missing out on seasonality. 

## Assumption 5: no perfect multicolinearity
Multicolinearity appears when one feature is a linear [[function]] of another independent variable. We can keep track of this by monitoring the *condition number* that `statsmodels` gives us when we run regressions. If we see a very high condition number, we can consider *partial least squares* or projection into latent spaces using [[PCA]]. We could also run a [[ridge regression]] or [[LASSO regression]] since the [[regularization]] would fix this problem. 