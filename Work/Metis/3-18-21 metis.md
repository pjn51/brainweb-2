# Metis Notes for Thursday, March 18th
#meeting/career

---
Today is the fourth day of [[metis week 11]]. We have a test on engineering, and then we have a review of language models and transformers. 

I have to finish my preparations for my mock presentation on [[wildfire classifier]] tomorrow. 

# Language models and transformers
In this review, we will cover language models, BERT, and transformers. 

A langauge model is different from a language. A language is a set of rules, while a language model is a probability distribsution over a sequence of words. We predict words as elements of a sequence. This is the task of the model. Most language models make use of a pre-trained [[neural network]]. 

Language models have tons of applications. They can summarize text, generate text in a certain style, and they power autocompletion. 

From around 2001 to 2013, NLMs and embeddings were the cutting edge. Word2Vec, GLoVE, and tools like that were the name of the game. This is when neural nets began to be used. These tools aren't full language models, but can peform [[embedding]] well. 

Between 2013 and 2018, we see advances to the field. Recurrent neural nets, LSTMs, Seq2Seq and ELMo are the tools of this era. [[attention mechanisms]] and context embedding were introduced as powerful improvements. More complicated neural nets were introduced, as well as [[transfer learning]]. 

From 2018 to 2021, we see the era of the [[transformer]] begin. GPT and BERT dominate the field. These are the largest language models, all built using the transformer architecture. 

## Transformers
LSTMs (Long Short-Term Memory models) were the big thing for a long time. They are a kind of recurrent neural network that has encoder and decoder layers. A RNN was used to build relationships between these layers. 

However, transformers are better. They are made up only of encoder and decoder layers, and do their own embedding at the first layer that takes in the text information. They learn context using the concept of *attention.*

Transformers have the advantage of being more parallelizable than RNNs because RNNs have to do things in order while transformers can run things simultaneously. Transformers are also computationally inexpensive and have constant time complexity. We can train state-of-the-art models in less than 12 hours. 

As I said, transformers are made up of *encoder* and *decoder* layers. An encoder layer maps a sequence of symbols to a sequence of context vectors. We're essentially transforming a vector into another vector at higher and higher levels of abstraction. Decoder layers generate sequences from an input, symbol by symbol. They take a sequence and try and generate another sequence, one symbol at a time.

However, there's a problem here. This system was designed for fixed-length sequence vectors, and sequences lose context when they exceed the max length. The solution is *attention.* Instead of using a moving window, we use a global weight matrix to relate each element in the sequence. This tells us how much we ought to care about every other element when it comes to considering the current element. Attention is the core innovation of transformers, and is the reason that they dominate the field right now. It eliminates 90% of the complexity of previous models. 

In the encoder blocks of the transformer, the info gets passed through the self-attention layer, and is fed to a feed-forward neural net layer in a 'straight line.' After being passed through all the encoder layers in a stack, the info is sent to all of the decoder layers simultaneously. The decoder layers also function in a stack, and after the [[data]] is moved through all those layers, we have our output. 

The transformer architecture dominates the field in 2021. BERT is Google's transformer, RoBERTa was built by Facebook on top of the BERT platform. GPT-3 was released by [[OpenAI]] in 2020. 

These models are truly huge. GPT-3 has 175 billion parameters, which is ten times more than any other model. This is actually overkill in almost any business setting, so these models are really just for research purposes. 

Now lets go over how we got to the BERT architecture. Before BERT, ELMo and GPT were the leaders in the field. ELMo was a LSTM architecture that used bi-directional features. This was inefficient, since it had to train two models. However, it was effective on many [[NLP]] tasks. GPT  is a transformer, trained with left-to-right token prediction. It leverages large corpus size instead of trying to create a novel architecture. This is because research has shown that training size is more important than architectural innovations. Therefore, the most efficient model will be the best, since we can train it on the most data. 

The only drawback of switching from LSTMs to transformers was that we lost bi-directional stuff. BERT fixes that issue. BERT stands for "Bidirectional Encoder Representations from Transformers." It was released in 2018 by Google. BERT gets good results driven by good training design. They make sure that the model can't see the word that it's trying to predict, but it can see the forward and backwards context. Basically, BERT is trained using a fill-in-the-blank approach. This allows for a lot of flexibility, and means that BERT needs less fine tuning than other models. 

The BERT architecture is made up of a simple structure. It doesn't have decoder layers, it only has a big stack of encoder layers. The BERT base uses 12 layers, 12 self-attention heads, and 110 million parameters. This is the same as the original GPT-1. The larger version of BERT uses 24 layers, 16 attention heads, and 340 million parameters. 

For more information about this stuff, look at the HuggingFace [[GitHub]] library. That's the standard for pre-trained transformers. It support PyTorch and TensorFlow. Jay Alammar has an illustrated article series on transformers. The SpeechBrain end-to-end speech toolkit is a great option for speech and audio processing using HuggingFace models. 