# Metis Notes for Monday, Feb 15th
`LINKS:` [[metis week 7]]
#meeting/career

---
I actually missed this day since I was in the hospital. These are my notes from watching the recorded lectures.

## Lecture - [[clustering]]
When we talk about clutering, we have to discuss the differences between [[supervised learning]] and [[unsupervised learning]]. When we perform supervised learning, we have a pre-determined number of classes that we're trying to categorize our [[data]] into. When we don't know how many irl classes there are, we can perform [[unsupervised learning]] to try and find out.

We also have to talk about [[classification]] versus clustering. Classification is to supervised learning what clustering is to unsupervised learning. Even if we don't know the concrete classes, we can cluster our data based on their attributes. After we have created these clusters on the training data, we can assign new data to whichever cluster most closely matches the new datum. 

One trick with clustering is knowing how many clusters to create. We have to apply our domain knowledge here. While there is no correct or incorrect quantity of clusters, there are better and worse ways to cluster. We want observations within a given cluster to be more similar to observations between adjacent clusters. 

There are a few metrics that we can use to grade our clusters. *Inertia* is the sum of the squared distances from each observation to the central point in the cluster to which that observation belongs. A lower inertia represents more densely packed clusters, which is usually a good thing. 
