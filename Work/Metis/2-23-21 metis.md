# Metis Notes for Tuesday, Feb 23rd
`LINKS:` [[metis week 8]]
#meeting/career

---
## Pair problem
We created a simple movie [[recommendation]] system using [[Pandas]]. 

One way to do this is the ==neighborhood method==. Vinny did this using [[dictionaries]]. One dictionary was for mapping between users and movies, and another was for mapping from movies to users.

The idea is that when we're given movie A, we want to return all the movies that the fans of A also liked. 

```python
def get_similar_movie(movie, n):
	biglist = []
	for u in movie_user_map[movie]:
		biglist.extend(user_movie_map[u])
	return [(film[a[0]], a[1]) for a in Counter(biglist)]
```

However there are limitations to this method. Users are likely to already have seen the movies that we want to recommend. It also tends to recommend very popular movies that most users would already know about, meaning that the recommendations aren't that groundbreaking. We also might miss some of the interesting underlying patterns that aren't visible with such a simple method. This is also a pretty slow method since we have a `for` loop. 

Another method would be the ==distance approach==. If we create a sparse matrix we can get some more out of this. If a user liked a movie, we will see a 1 at the intersection of that user and movie.

Now we can calculate the [[euclidian distance]] between movies, and just return the three movies that have the shortest distance between them and the one that the user just finished. 

There are limitiations to this approach as well. The first issue is that we will end up recommending *very* similar movies that might actually be too similar. For example, *Die Hard* would just recommend *Die Hard 2*, and so on. 

An even bigger problem is that this method tends to treat unknowns as information. It will think that if a user didn't like a movie, they must have disliked it, when in reality they probably just haven't seen it. 

In order to get around this issue, we could use a different distance metric that ignores zeroes. 

Another approach would be to implement ==NMF and distance==. By reducing dimensions, we can get some components from our sparse matrix. This is the most interpretable method since we could name the components which might be related to the genre or something. 

## Investigation - anomaly detection
- Anomalies occur infrequently but can mess up results
- What is an anomaly
	- Pattern in the [[data]] that doesn't conform to the overall pattern
	- Not "noise"
	- Often indicate important things happening that aren't well described
- Kinds of anomalies
	- Global
		- Most common type
		- Deviate from all the other data
	- Contextual
		- Deviation is only strange because of the context. A warm day in winter would be an example
	- Collective
		- A whole collection of anomalies that tend to cluster together
- Machine learning approach
	- Supervised anomaly detection
	- Semi-supervised AD
	- Unsupervised AD
- Novelty detection
	- We predict based on little prior information with this approach
	- Looking for unusual events 

## Lecture - [[NLP]] review
- Workflow steps
	- Text list
	- Vectorizer
	- NMF components between 5 and 20. Decide based on top words. 
	- Cluster or predict in some way if that's what you're going for
- What if you don't want to cluster?
	- We can use the topic distributions in any way we want. 
	- Or we can use these as vectors themselves and aggregate them for a recommendation engine of some kind. 
- Size of the document
	- If you're looking at books, your document could be defined as the whole book, a chapter, or just a sentence. 
	- How does this change things?
	- If we end up just aggregating the observations anyway, it doesn't really matter what level we have our documents defined as.
	- There's a catch!
	- In the real world, topics get messed up. When we want to model resturant reviews, the topics don't end up as 'food', 'service', 'atmophere'. Instead they end up as 'asian', 'italian', and 'burgers'. We can avoid this by changing the scale at which we are aggregating. 
	- If we model each resturant individually, and then translate 'burgers' to 'food' (and so on), we can get more insight across multiple types of resturants.  

## Lecture - CorEx [[topic modeling]]
So far, we've been trying to generate new content, using *generative models.* CorEX does this in more of a reductive way. We're just trying to get the topics given the corpus of documents.

CorEx uses correlation to do this. It bunches up words that commonly occur together. We find total correlation, which is the sum of the entropy of each word, minus the entropy of the group of words. 

Entropy is just the number of times that the word appears. So if a word A almost never appears, but when it does it almost always goes along with word B, total correlation between those words will be high. This allows the model to tell if the words really belong together, based on the proportion of the time that they are occuring together relative to how often they occur sepreately. 