# Metis Notes for Monday, Feb 22nd
`LINKS:` [[metis week 8]]
`TAGS:` #meeting/career

---
## Pair Programming - markov chains
The idea of a Markov chain is to model the next word in a sentence dependent only on the previous words. This isn't going to be the best way to model, but it is kind of fun.

We're going to apply this idea to some inspirational quotes which should give us some amusing results.

```python
from collections import defaultdict

def markov_chain(quotes):
    mydict = defaultdict(list)
    tokenizer = RegexpTokenizer(r'\w+')
    
    for quote in quotes:
        #tokenize quote
        tok = tokenizer.tokenize(quote)
        #print(tok)
        
        #for i in range(len(tok)):
            #mydict[tok[i]].append(tok[i+1])
            
        for i, word in enumerate(tok):
            try:
                #print(i, word)
                nextword = tok[i+1]
                #print('next word: ',nextword)
                mydict[word].append(nextword)
            except:
                continue
```

We first have to do text preprocessing, such as getting rid of the names of each author and the punctuation. Then, we can tokenize all the words in each quote and prepare our function.

We want to iterate through each quote, and then within each quote, we want to examine each word of the quote. 

If the word hasn't yet appeared, we want to add the word as a key into the dictionary, and keep track of what word followed our target word.

If the word has already appeared, we want to take the next word and add it to the list of words that follow this word in the corpus.

We can do this using a `defaultdict` so that when we attempt to pass in a new key, the dict will automatically create a new key with a defined default value (an empty list in our case). 

```python
def markov_chain(corpus):
	words = words.lower() for word in clean_text if word != '-'
	
	word_freqs = defaultdict(list)
	
	for current, following in list(zip(words, words[1:])):
		word_frequencies[current].append(following)
```

From here, we can actually create a text generator that can create new sentences. 

We want to randomly choose the first word and capitalize it. `random.choice()` will give us a random element of whatever we pass into it. We can pass in the keys of our dictionary in order to get a random word from that set of words. We have to pass `random.choice(list(words.keys())` because we need to view the keys as a list instead of a set to do a random choice operation.

Now we need to randomly choose a value associated with the random key we chose for the first word. If we create a new variable to store the random starting word, `start`, we can do this by passing `random.choice(words[start])`. This will randomly choose one of the words that follows our start word in the corpus. 

Here's the final function that we can generate sentences with.
```python
def sentence_gen(words, sentence_len):
	sentence = []
	
	start = random.choice(list(words.keys()))
	
	for i in range(sentence_len):
		next_word = random.choice(words[sentence[1]])\
		sentence.append(next_word)
		
	sentence[0] = sentence[0].capitalize()
```

## Lecture - advanced [[clustering]] algorithms
First, let's review what we mean by clustering. We group "nearby" observations, and define closeness by distance or density. We can perform [[k-means]] clustering, which is based on distance to nearest centroid (central point of a cluster). 

However, K-means is sometimes not the best solution. If we use a sub-optimal value for *k*, we can get sub-optimal results. And it's difficult to determine what the best value for *k* really is. Additionally, we make a lot of assumptions about clusters when using k-means. K-means can only handle clusters that have a gaussian distribution (see [[data and distributions]]). This means that if we're working with three features, the clusters have to be spherical for k-means to work well. 

There are other options that we can use. We could use *hierarchical agglomerative clustering (HAC)*, or we could use *DBSCAN* or *mean shift*. We can also [[model validation|validate]] our technique in a better way if we use an external criteria. Clusters can't validate themselves. 

### Hierarchical Agglomerative Clustering (HAC)
HAC is a partitioning algorithm that merges [[data]] sequentially using a metric such as [[euclidian distance]]. It creates fewer, larger clusters than k-means does. The algorithm ends at a given number of clusters, or a specified distance between the clusters. 

The algorithm actually works by starting with each data point as its own cluster. Then, the most similar clusters are merged together. The process repeats until one of the stopping conditions are reached. 

For [[hyperparameters]], HAC takes the metric for calculating distance, as well as a linkage process (the process for how to merge nearby clusters). For linkage processes, we could use minimum distance, maximum distance, or average distance between points in a cluster. We also have to specify an end condition for HAC. 

We don't have to specify the number of clusters right away, but we can do so using domain knowledge. If we don't have that information, we can use inertia or the *sillouette coefficient* but these aren't perfect metrics.


| Strengths                                  | Weaknesses            |
| ------------------------------------------ | --------------------- |
| Easy to interpret                          | Slow runtime $O(n^3)$ | 
| Easy to use                                | Many decisions        |
| Don't have to decide n clusters right away | Hard to get robust    |

### DBSCAN
DBSCAN is a very computationally expensive algorithm. It becomes more robust once we run it. DBSCAN stands for "density based spatial clustering of applications with noise." 

First, we cluster points using density of "local neighborhoods" in the data space. We find the core points in high-density areas, and then expand clusters around those points. In this algorithm, points can belong to no clusters. 

For hyperparameters, DBSCAN takes a distance metric, $\epsilon$ (the radius of local neighborhoods), and `N_clu` (determines density threshold). 

DBSCAN will categorize a point as a core point, a density-reachable point, or noise. A core point is a point that has more than `N_clu` neighbors in their neighborhood. A density-reachable point is a neighbor of a core point that has less than `N_clu` neighbors, and a noise point is a point that isn't nearby enough points to belong to a cluster. 

| Strengths                         | Weaknesses                       |
| --------------------------------- | -------------------------------- |
| No need to specify cluster #      | Computationally expensive        |
| Can handle non-spherical clusters | Requires 2 params                |
| Allows for noise                  | Struggles w different densitites |
                            
### Mean Shift
Mean Shift is another partitioning algorithm. It's sort of like k-means but it uses density instead of distance. We determine centroids based on the points of highest density. This algorithm doesn't allow for noise, and puts all observations in one cluster or another. 

The algorithm begins by starting with a random point and a window. It calculates a weighted mean for the window, and then shifts the center of the window to the new mean. It repeats this process until *convergence* occurs. Convergence means that there's no change in mean after the latest shift of the window. We repeat the above process for each observation. Data points that end up at the same mode go into the same cluster. 

For hyperparameters, Mean Shift takes a weighted kernel [[function]], and a window size. A larger window size gives us more homogenous groups. 

| Strengths                                   | Weaknesses                    |
| ------------------------------------------- | ----------------------------- |
| Model-free (no assumption of cluster shape) | Result depends on window size |
| Just one parameter (window size)            | Hard to choose win size       |
| Robust to outliers                          | Slow $O(mn^2)$                |

### When to choose what model?
DBSCAN does the best overall in the most situations but it is computationally expensive and it might take *hours* to run. DBSCAN also allows for noise, so it is more robust in situations where you don't want *every* data point to be assigned to a cluster. 