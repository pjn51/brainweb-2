# Metis Notes for Tuesday, 1-12-21
`LINKS:` [[metis week 2]]
`TAGS:` #meeting/career

---
Today we will start with an extended lecture on the theory and application of linear regressions. After lunch, we will have a pair programming exercise that has to do with what we have learned. 

# Grading policies for our projects
A 15/25 is a *solid* score. Don't be scared if that happens. 

# Theory of [[linear regression]]
To begin our dive into linear regression, we have to understand just what a [[machine learning]] model really is. In Jewish folklore, the golem is a clay robot brought to life by magic. It always does precisely what it's told, which is a blessing and a curse. Often, the golem's instructions have unintended consequences. The models we use are like golems. They have immense power and can be really useful, but only if we tell them what to do very specifically. 

There are various ways that we can think about our [[data]]. We can think of it in a *graphical way*, or in a *tabular way.* If we're dealing with height and age as our features, we could put this [[data]] into a table, or we could see it as a scatterplot. The goal of dealing with this data is to discover relationships between features, and use those relationships to make predictions. 

The simplest way to quantify a relationship is with a linear equation in the form $y=mx+b$. This represents the line of best fit. When we search for this line, we want to find the line that is closest to all the observations. We need to measure the distance between each point and the point on the line of best fit that we predict that point to be at, and minimize this distance, which represents the *[[residuals|residual]].*

This process is formalized as [[RMSE]], which can be represented as the following equation. 

$$\frac{\sum{\sqrt{(y_p-y_a)^2}}}{n}$$

- $y_p$ = predicted value
- $y_a$ = actual value
- $n$ = number of observations

We could use absolute value instead of squaring the difference and then taking the root, and that would give us [[MAE]]. MAE is more useful when you're looking at the real use case, because it will tell you how much you're actually off by. For example, if you were trying to predict housing prices, MAE would tell you how many dollars you were off by. 

While we can think of the line in the form $y=mx+b$, we usually use a different notation. It's more common to see $y=\beta_0+\beta_1x$. This is more useful since we can add more features (more betas) easily. 

$$y=\beta_0+\beta_1x+\beta_2x_2+...+\beta_nx_n$$

There are two ways to interpret the coefficients: counterfactual and predictive. Using the counterfactual approach, all else being equal, if I increase age by *x*, height will go up by $f(x)$. We would interpret $\beta_1$ this way. This is a hard claim to make, since it implies *causaility.*

Using the predictive approach, we ask what the value of $f(x)$ be when *x* is zero. This is how we would interpret $\beta_0$. This isn't making a causal claim and is therefore easier to assert.

If we want to stay away from causality, we can just compare between groups. We shouldn't say that a change in the coefficient is associated with a change in *x,* since this is a little too close to impying causality. 

[[data science]] refers to *features and targets.* This is just different lingo for the independent and dependent variable. 

Now let's dive into linear regression directly. Most relationships are *not* perfectly linear, so most linear regressions are just approximations of more complex systems of behavior. 

During the preprocessing step, there are different kinds of variables that we might run into. *Continuous  variables* are a common numeric variable. They can be any value, and are represented by [[floats]] in [[Python]]. We could consider *standardizing* these variables. To standardize a continuous variable, we subtract the mean from each individual observations, putting them in terms of a [[z-score]].

$$\frac{a_i-a_m}{stdev(a)}$$

- $a_i$ = each observation
- $a_m$ = the mean of all the observations

This helps us to interpret the y-intercept as the [[average]] prediction. 

*Categorical variables* are usually [[strings]] in python. We have to *encode* these variables so that the machine can read them. One way to do this is through *one-hot encoding* or the use of *dummy variables.* We take the strings and create an indicator variable that converts the strings to numbers. For example, if we had a sex dataset that had a column indicating gender, we could replace the single column containing "M", "F", or "Other", with two columns. One column would be called "male" and would have a 1 if the observation was a male, and 0 if it wasn't. We would do the same for "female." We can use two columns for the three possible states, since an observation with zeroes in both the male and female column would be assumed to be "other." We remove the third column not only to save on space and time, but to avoid *multicolinearity.* This happens when we have multiple columns representing the same information. It's bad because it leads to instability in optimization.

# Coding LinReg with continuous variables
For more information, see the `linear_reg_code.ipynb` file in my repo. We can just pass `df.corr()` to see correlations between every feature, but this is hard to interpret. Using [[seaborn]], we can create a heatmap for our dataframe `df`.

```python
fig, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(df.corr(), cmap="seismic", vmin=-1, vmax=1, ax=ax);
```

We could also do a scatter matrix to see a broad view of how all the features stack up.

```python
fig, ax = plt.subplots(figsize=(10, 10))
pd.plotting.scatter_matrix(df[df.columns[-10:]], ax=ax);
```

After we see all out features like this, we can zoom in on which features we would like to create a linear regression for. We're working with a housing dataset, so let's zoom in on house price vs lot area. First we have to subset the df, making sure that we do double brackets around lot area, so that it changes from a series to a dataframe (good tip).

```python
SWISU_x = data[['LotArea']]
SWISU_y = data['SalePrice']
```

Now we can plot the scatter plot to look at what we can see.

```python
ax.scatter(SWISU_x, SWISU_y, alpha=.5)
ax.set_xlabel('Lot Area')
ax.set_ylabel('House Price')
ax.set_title('House Price vs Lot Area in the South & West of Iowa State University Neighborhood')
```

And we can run a linear regression on this relationship. We simply instantiate a linear regression from the [[Scikit-Learn]] module's class just for this purpose. Once the linreg exists, we use the `.fit(x,y)` method to fit the LinReg to our data. We can see what the coefficient of determination, $R^2$ is through the `.score(x,y)` method. 

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(SWISU_x, SWISU_y)

lr.score(SWISU_x, SWISU_y)
```

$R^2$ is the [[coefficient of determination]].

$1-MSE$ or $$1-\frac{\sum{\sqrt{(y_p-y_a)^2}}}{n}$$

When $R^2$ is one, that means the LinReg perfectly captures all the variation in the data. A score of zero means that your model is as good as just guessing the mean.

Don't get tunnel vision about this metric. There is no standard value for $R^2$ that we're trying to hit. Even if the $R^2$ only explains 17% of the variance, that might be awesome if it's the best model ever created. 

Another metric we can use is MSE. We plug in the predicted values for x that we find through `lr.predict(x)`.

```python
from sklearn.metrics import mean_squared_error

pred = lr.predict(SWISU_x)

np.sqrt(mean_squared_error(SWISU_y, pred))
```
`pred` creates predicted y values based on the test x values. This will result in a number that tells us how far off our predictions are, on average. However, this will just be a raw number. So the number might be 5,000, but that could be a huge failure or a huge sucess depending on the scale of the numbers that we're working with. 

We find that the slope here is 5.30, and the y-intercept is 99498. We could interpret this as a comparison between two groups. If there were two groups of houses that were totally the same except for 1 sqft difference, we would expect the average difference in price to be $5.30. We would also say that we would predict the price of a house with 0 sqft to be $99498. This obviously doesn't make any sense but that's what the regression says. 

# Coding a LinReg with categorical variables
In order to work with categorical variables, we have to perform one-hot encoding. We can use the `OneHotEncoder` object provided by sklearn. There's also a tool through [[Pandas]] that we could use, called `pd.get_dummies()` but this tool fails on certain edge cases. If the training data doesn't have all the categories that the testing data has, then the tool will fail. 

The housing data has a building type column that has five categories. We can create a `OneHotEncoder` object, specifying that it should drop the first column. We can fit it to the subset of the housing dataframe that we call `cat_X`. We tell the one-hot encoder where to get the column names by specifying `columns = ohe.get_feature_names(['BldgType])'`, and then we put the final result into a dataframe.

```python
ohe = OneHotEncoder(drop='first', sparse=False) # instantiate object

ohe.fit(cat_X) # fit object to training data

ohe_X = ohe.transform(cat_X) # transform the training data using the ohe

columns = ohe.get_feature_names(['BldgType'])

ohe_X_df = pd.DataFrame(ohe_X, columns=columns, index=cat_X.index)
```

Now that we have a one-hot encoded dataframe, we can run a linear regression using these features.

```python
cat_lr = LinearRegression()

cat_lr.fit(ohe_X_df, cat_y)

cat_lr.score(ohe_X_df, cat_y)
```

This will give us a score just using these new features, but we should go ahead and combine the categorical dataframe back into the other dataframe with the continuous variables. We can run a linear regression on all of them and probably get a better result.

```python
combined_df = pd.concat([ohe_X_df, df[['LotArea']]], axis=1)

combined_lr = LinearRegression()

combined_lr.fit(combined_df, cat_y)

combined_lr.score(combined_df, cat_y)
```

It turns out that when we include property type, we actually confuse the model. The combined model has a $R^2$ of 0.102, meaning that it explains 10.2% of the variance, while the one just focusing on lot area explains 17% of the variance. 

Let's investigate what's going on here. We can get the coefficients and the intercept by passing `combined_lr.coef_` and `combined_lr.intercept_`. Using this, we can see what effect each feature had on the predicted price of a home.

We've neglected a crucial step in the process here. We've just been training on the whole dataset, when really we should be performing a train-test split. Remember to do that when actually performing linear regression.

