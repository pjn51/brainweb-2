# Notes for Tuesday, Jan 26th
`LINKS:` [[metis week 4]]
#meeting/career

---
Today we're starting off with a review of the second [[linear regression]] test we took yesterday. Then, we have lectures on [[logistic regression]] and a review of [[SQL]]. 

## Regression assesment II review
- Q9: This was a question about the effect multicolinearity has on our models. 
	- Multicolinearity makes model coefficients harder to interpret, while not affecting model accuracy. 
- Q15: This was a question about creating a model that predicts a music streaming user's total listening time per week.
	- We don't have to exclude newer listeners from the model, even though there is less [[data]] on their listening habits. 
- Q17: This was a question about a newspaper distribution model. 
	- Stores should be sent more papers each day than the model predicst they will sell, to prevent the significant downside of sellouts, with the buffer determined via cost-benefit analysis and applied on a per-store basis. 
- ==Autoregression==: if a feature is regressing on the target itself, that feature is autoregressive. For example, if we want to predict future [[stock]] prices, an autoregressive feature would be the stock price a week ago. 

## Ridge family time
- We're continuing our discussion of the workflow
- Iterating
	- Make a simple, dumb model first.
	- We have to justify complexity in terms of better outcomes. 

## Lecture: [[logistic regression]] theory
What if we want to map a continuous variable *X* to a success indicator, *Y* which is [[binary]] (either 0 or 1)? We can use a logistic regression.

To *regress* is to tend towards a less established, previous state. In *regression,* we model tendency towards a less true expected value. Given a value *X,* we expect some central tendency of *X.* This implies that *X* is normally distributed. We assume that the real set of *Y* for a certain *X* value will be normally distributed around the expected *Y* value that our model tells us.

For example, take basketball. If we have our *X* feature as "distance from the hoop," and the *Y* success indicator is whether the player made a basket, we can see that there's a special relationship here called the *Bernoulli distribution* (see [[discrete and continuous variables]] for an explanation). Given *X,* we want to find the probability that *Y* = 0 (aka that the player missed the shot). 

For [[linear regression]], our model took the form $y=\beta_0 + \beta_1x_1...\beta_nx_n$. We need a *link function* to connect this model with a probability statement. 

Many functions could be used for a link [[function]]. When we deal with the Bernoulli distribution (see [[data and distributions]]), we usually use a [[sigmoid]] function. 

$$ p = \sigma(z) = \frac{1}{1+e^{-z}}$$

*Z* can be any number between zero and one. We can rewrite the above to read...

$$z = log(\frac{p}{1-p})$$

This means that *Z* represents the [[log odds]] that *Y* = 1. 

### The Logit
*The logit* maps a probability to its log-odds. 

$$z = logit(p) = log(\frac{p}{1-p})$$

In linear modeling, *Y* is the *conditional expectation.* In logistic modeling, *Z* is the log-odds. The inverse of this log-odds gives us our condtional expectation.

The logit has a lot of synonyms, including:
- Inverse logit
- [[logistic function]]
- Expit 
- Sigmoid

### Solving the basketball example
First, we would fit a linear model that gives us a continuous outcome. Then we have to determine the bounds of *Z.* For this problem, we want *Z* to be between zero and one. This means that we take *Z,* representing the log-odds, and invert the log-odds function to find a sigmoid function. We can use this sigmoid function to transform our linear model. 

- Steps to solve the basketball ex
	- First, find a linear model that gives us a continuous outcome
	- Now we have to determine what $z$ is. For this problem, $z$ must be between 0 and 1. 
	- This means we take $z$, which will be the log-odds of what we want. We invert the log-odds function to find a sigmoid function. 
	- We can use this sigmoid function to transform our linear model. 

## Lecture: logistic regression code
See the `logistic-regression.ipynb` notebook in my repo. 

We're going to look at a small housing dataset here for SF and NY. We will perform simple logistic regression using one feature, and then multiple logistic regression using multiple features.

Import step, getting the data.
```python
import pandas as pd
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import statsmodels.api as sm

df_housing = pd.read_csv('./ny_sf_apt.csv')
```

For simplicity's sake, we're going to split the data upfront. 

```python
df_train, df_test = train_test_split(df_housing,
                                     test_size=0.2, 
                                     random_state=42)

```

Here we are creating a simple logistic regression using statsmodels. 
```python
# For this first example, we'll employ statsmodels
lm_1 = sm.Logit(df_train['in_sf'],  # with statsmodels, `y` comes first
                sm.add_constant(df_train[['price_per_sqft']]))  # and then `x`
lm_1 = lm_1.fit()

lm_1.summary()
```

We use the final line to look at the performance of the model. This tells us a few important things, like ==log-likelihood==, which is the [[logarithms|log]] of the probability that we would see this pair of data (x,y). Ideally, this would be *zero*. 

Logistic regression actually gives us the probability of a house being in SF or NY. This is called *soft classification*. We can set a cutoff to determine where this dividing line falls. The default is 50%. After we set this cutoff, we have established a *hard classification*. 

Once we've fit the model, we can create a confusion matrix. This will return an array that shows the predicted value and the real value. 

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(df_eval['in_sf'], df_eval['pred'])
```

Now we want to find what the error does as we change the threshold of the decision regions. We do this using a plot of the ==receiver operating characteristic==. 

```python
from sklearn.metrics import roc_auc_score, roc_curve

fpr, tpr, thresholds = roc_curve(df_eval['in_sf'],
                                 df_eval['proba_sf'])

def plot_roc(true, probas):
    auc = roc_auc_score(true, probas)

    plt.plot(fpr, tpr, marker='o')
    plt.xlabel('1 - Specificity (FPR)')
    plt.ylabel('Sensitivity (TPR)');
    plt.title(f"Area Under the ROC Curve: {round(auc, 3)}");
	
plot_roc(df_eval['in_sf'], df_eval['proba_sf'])
```

## Lecture: advanced [[SQL]]
- We have to write commands in ways that the relational databases understand. 
- Not a [[programming language]] itself, it's just a query language. 
- [[SQL]] kind of isn't useful for our purposes, but we need to know how to use it for our careers. 
- The reason this stuff is advanced is because it isn't intuitive. [[SQL]] doesn't read stuff from left to right in our query which is messed up.
- Overview
	- Today we'll talk about partitions
	- Joining
	- `WINDOW` functions
- PandaSQL
	- This is a way to connect our jupyter environment to SQL. 
	- It allows us to enter queries and pull things from dataframes. 
- `CAST`
	- A SQL function that allows us to put a thing to another thing.
	- We could cast an int to a float, or change the way a date-time is written.
	- DOUBLE PRECISION is a specific type we can change something to. A different numeric type. 
	- `CAST var_1 AS DOUBLE PRECISION / var_2 AS var_1_div_2` would change variable 1 to a double precision type, divide it by variable 2, and make a new variable for that result. 
- `WITH`
	- It's annoying but the syntax is kind of backwards here. If we want to make a certain < selection > and call it 'sel1' we would query `WITH sel1 AS < selection >`
- Window functions
	- I totally lost the thread here. See my notes in the SQL note on window functions. 

## Working through `Advanced_SQL.ipynb`
- I totally lost pace with the lecture, so I'm just going to work through the notebook at my own pace here.
- Window functions in SQL overview
	- We use a window function when we need to use information from another row to calculate a value for the current row.
- Getting the data in df and postgres
	- Of course, we can just read it into [[Pandas]] using `df = pd.read_csv()`
	- If we want to read the data in from a [[database]], it gets more complex. 

```python
from psycopg2 import connect
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT

params = {
	'host': '18.220.115.81',
	'user': 'ubuntu',
	'port': 5432
			}

# connect and create database, disconnect, and reconnect
# to new database
connection = connect(**params, dbname='ubuntu')
connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
connection.cursor().execute('CREATE DATABASE store;')
connection.close()
```

Here's a different way to create tables which is convinient if there are many columns. 
```python
from sqlachemy import create_enging

connection_string = f'postgres://ubuntu:{params["host"]}@{params["host"]}:{params["port"]}/store'
engine = create_engine(connection_string)
df.to_sql('sale', engine, index=False)

connection = connect(**params, dbname='store')
cursor = connection.cursor()
cursor.execute("SELECT * FROM sale;")
cursor.fetchall()
```

What if we have some salary data, and we want to find the salary of each person as a fraction of the highest paid person in that department?

Here's how we would do this in [[Pandas]].
```python
highest_dept = df.groupby('dept').salary.max()
highest_dept = pd.DataFrame(highest_dept).reset_index().rename(columns={'salary': 'highest_salary_in_dept'})
```

Here's how we would do it in SQL (but still within python) using GROUP BY / JOIN.
```python
query = """
WITH dept_info AS (
  SELECT dept, max(salary) AS max_salary, sum(salary) as salary_budget FROM sale
  GROUP BY dept
)

SELECT sale.*, max_salary, CAST(salary AS DOUBLE PRECISION)/max_salary as max_frac  
  FROM sale LEFT JOIN dept_info ON sale.dept = dept_info.dept;
"""

cursor.execute(query)
cursor.fetchall()
```

We could also use a window function, like so.
```sql
SELECT *, MAX(salary) OVER (PARTITION BY dept) FROM sale;
```

TO BE CONTINUED...

## Lecture: [[GCP]] setup
GCP stands for Google Cloud Platform. We can use GCP for projects that our computer can't handle, like deep learning and large neural networks and stuff. 

It's free, but you have to pay eventually. See [this](https://github.com/thisismetis/onl_ds5/blob/main/curriculum/project-03/gcp-setup/gcp-setup.md) tutorial for setting up a GCP virtual machine. 

This isn't required for any project, it's just a tool that might be useful. 