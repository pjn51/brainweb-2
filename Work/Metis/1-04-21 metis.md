# Metis Notes for Monday, Jan 4th
`LINKS:` [[metis week 1]] // [[1-05-21 metis|next]]
#meeting/career 

---
On the schedule for today is an intro by the Metis admin team, where they will discuss a lot of ground rules and procedures for the course. 

## Intros
Caroline Csernus - Program Manager for the Ridge group. Living in Chicago. Fun fact: severely left handed, but can only snap with the right hand. 

Rita Biagioli - Instructor. PHd in cultural anthropology. Taught [[data]] stuff at a few places. Lives in NJ and Chicago. Loves tea. 

Brian McGarry - instructor. Studied chem and philosophy. Worked at sports analytics startup. Loves teaching. Was a gymnastics instructor. 

Lucy Abbot - TA. Lives in SF, background in consulting. Likes sourdough fermentation. 

Chris Doenlen - TA. Background in finance. From TX. 

Sara Zong - SF. MS in stats. 

Young Suh - From Cali. Has a Cog Sci degree. Used to dance a lot. 

Grettel Juarez - From San Diego, CompEng degree, 12 yrs consulting. Likes crossfit. 

Satenik Safaryan - From Romania, now in SF. Hates winter. Major in business admin. 

John Metzger - Goes by Derek! Lives north of SF. pHd, plays music, managed a farm. He met Bill Nye in an elevator once. 

Amy Butler - In Northen VA. Background in econ and stats. 5 yrs in consulting. Hates to cook!

Jess Martin - In Chicago. Arabic translation background. Spent a year in Egypt right after the revolution which was awesome. 

Michael Jehl - In Chicago. Into growing mushrooms. 

Alba Sitdikova - From Russia, now in LA. 

Will Moore - In Chicago. 10 yrs in craft beer. Now GM at a brewery. Plays darts. 

Will Noble - From VA, now in AZ. 

Anubhav Parkeet - From India. BS in CompSci. 

Gabriel Equitz - BS in CompSci. In SF. Met Gavin Newsom once on a train. 

Jesus Barela - Goes by Aaron. In LA. CompSci for game design. Former semi-pro for counterstrike!

Nathaniel Speister - In Boulder CO. Masters in physics. Has lived in all US timezones. 

Yuwen Huang - In California. College stats. Loves sports but not very good. 

Matt Segall - In Oakland. Econ and jazz. Played in a giant funk band in Ohio. 

Mariam Yousef - San Hose. Econ. Has a cat. 

Emma Claire - Molec bio degree, in Oakland. Used to pole vault. 

Kevin Liang - From China, now in Newport Beach. Has a twin sister. 

Kaitlyn Zeichick - Grad with cog sci degree. In Chico CA. Likes to climb mountains (once during an earthquake)

Katie Huang- From Taiwan. Lives in Santa Barbara. Grew up with 4 cats, now has 2 kitties. 

Alexander Katz - From SF / Santa Criuz. Mech eng degree. Once built fences in highschool.

Amy Sillman - In SF, from Ohio. PHD in immunology. Worked in biotech.

Warren Lee - Econ. In SF. Streams on twitch. Learning how to ice skate. 

Me - Michael likes mushrooms, comment on that. Fun fact - ran a research project on the interactions between soil [[fungi]] and plant pathogens where I grew and intentionally killed hundereds of raspberry plants. 

# Let's get started!
All 4 instructors will be doing lectures, since both sessions are together for that. Rita is starting us off today. 

We should include 'ridge' in our zoom names. 

[[Data science involves coding, math, and communication]]. To be a good data scientist, you have to exist at the intersection of these skill areas. You have to have proficient hacking skills, math and statistical knowledge, and expertise in the field that you are working in. 

There are three main skill domains of [[data science]]. Math and [[machine learning]] comprise the first, while programming and communication are the second and third. All are important and we should seek balance between them.

A good data scientist is curious, creative, honest, humble, and has grit.

Time in this course will be broken up into focused work time, peer and community support, interactions with instructors, and career support. 

This bootcamp is modeled to have three components - training, repetition, and culture / community. The training component is based on the idea of the OSEMNC process. This is an acronym standing for Obtain, Scrub, Explore, Model, iNterpret, and Communicate. This is not a linear process, and we will do some jumping back and forth as need be during each project. 

The tools we will use throughout this course consist of a set of python tools like `pandas`, `numpy`, and `git`, as well as statistical tools such as `statsmodels` and `scikit-learn`. We will use `beautiful soup` and `selenium` for [[web scraping]]. For saving files, we will use flat files, postgreSQL, and [[MongoDB]]. We will also investigate Amazon Web Services, as well as things to help us work online, like HTML, CSS, and [[flask]]. Always remember that Google search is your best tool!

Repetition is the way to learn concepts well. We will have five projects that each have elements of design, data, algorithms, tools, and communications within them. These projects will get more and more independent as time goes on. The first project takes one week, while the final project takes 4. 

The unit of repetition is the practice of doing [[data science]]. 

When we become data scientists, we have to overcome the twin vices of impostor syndrome and perfectionism. We overcome the former through group work and communication, and the latter through unfairly short deadlines. 

We all come from different backgrounds, so don't compare yourself to others. What is easy for you may be hard for others, and vice versa. 

## Pair programming
Metis tries to incorporate moments of collaboration. This skill is important for job [[interviews]], for your career, and helps us learn from and teach others. Pair programming is a bit part of this. One partner will be the driver, working on the code and syntax, while the other is the navigator. The navigator thinks about the big picture strategy of answering the question. We switch roles every day to get skilled at both. 

The goal of pair programming is to solve the problem obviously, but also to teach and be taught by your partner. If one person is doing all the work, you're doing it wrong. 

Pair programming sessions will last 30-45 minutes, and then we will reconvene to share our solutions. 

# Policies
In order to graduate, we have to meet two requirements. We have to have met the attendance minimum, and we have to have a decent score for our deliverables. 

The local team consists of two instructors, Rita and Brian. There are also 2 TAs, Lucy and Chris. Caroline is the project manager for my group, Ridge. We are encouraged to schedule a 1-on-1 with instructors or TAs each week. 

## Attendance
Attendance must be at least 80%, and we can't miss more than 4 days in a row. If we're more than 30 min late, that counts as a tardy. Six tardies add up to one absence. Concretely, we can miss no more than 8 days during weeks 1-8, and no more than 4 days in weeks 9-12. If we are going to miss more than that in the latter period, we can request a Leave of Absence. 

## Schedule overview
Each day will start with pair programming for an hour, followed by project time and lecture. Then we will break for lunch. The afternoon session will be [[career]] related or flex time. There are a few hours for 1-on-1s or project time at the end of the day. 

## Scoring
There are five scoring categories in each project:
1. Design
2. Data
3. Algorithm
4. Communication
5. Tools

Each category has 5 possible points, and we need to maintain an [[average]] of at least 12/25 throughout the course. 

## Career support
There is a career support channel in slack. There are 2 career advisors, Jennifer Maimone and Ashley Purdy. 

# Lecture - Data cleaning, EDA, Pandas
We have a few afternoon lectures this week but that's only for the first week. 

## Data cleaning
`LINKS:` [[data wrangling]]

Data cleaning takes a lot of time. The workflow is nonlinear. Sometimes we have to do some EDA or model, and we realize the data needs to be cleaned again so we have to go back. 

We have to clean data because it's messy. It can be biased and is often inperfect. Since our algorithms are fed by the data, they can only be as accurate as the data is. 

When we find repeat observations, we can do a few things. What we do depends on what we're using the data for. We should ask why the duplicates are there, and remove any unnecessary crud around the data we're really after. 

If the dataset is too big to easily see repeats, we can use [[summary stats]] to check for red flags, and we can look at the unique values of categorical data. 

When data is missing, we can either remove the observation or make an educated guess about what should go there. Maybe we could fill with the average or [[median]] value in that column for instance. 

As for outliers in the data, we can't simply remove them without an investigation. They might be true, but they might also be obscuring the real pattern at work in the data. In order to find these, we can plot the data, or we can use statistical methods and [[residuals]] to find them. Once we find them, we have a decision to make. If they aren't screwing much up, we can leave them. But if they are, we should remove them and replace them with a more appropriate value (*this seems shady to me*).

Whatever we do, we need to **keep a record of methodology**!

## [[exploratory data analysis|EDA]]
We use [[exploratory data analysis]] to get a feel for the data. We can also identify patterns and trends, and EDA provides valuable context for models and for communicating our results. Most people know what an average is, but don't know about more complex tools. 

We first turn to summary [[statistics]] like average, maximum, and minimum. We also investigate correlation. We also use visualization as part of the EDA suite. In order to do this, we are often working in python tools like [[Pandas]], [[matplotlib]], and [[seaborn]]. We also use plot.ly. 

## [[Pandas]] basics
For more information, see the `pandas-revisited.ipynb`file in the onl_ds15 repository. 

When we want to **read data into pandas**, we can convert from a csv file using `pd.read_csv()`. This method takes either a file path from the local computer, or a URL leading to a csv. Once we have the data read into pandas, we should use `df.head()` to make sure everything is right. It's also very useful to read the [[metadata]] in order to know what any abbreviations mean. When we're reading the data in, we can specify what null values are indicated by, using `pd.read_csv(na_values=[])`. This is useful if the creator of the data used something weird to indicate a NA. This method returns a dataframe object. 

If we want to grab a row out of our dataset, we can use `pd.df.loc['index name']` or `pd.df.iloc[index #]`. 

Once the data is read in, we can explore patterns a little bit. `df.describe()` is a quick way to pull a bunch of summary statistics from the numerical columns. We can use `df.value_counts()` to count up the number of times each observation is represented in the dataset. 

If we just want a subset of the data that meets a certain criteria, we can use a **mask**. A mask is just a logical statement, like `mask = (df.some_value > 5)`. We can apply this mask to our df. So that means `df[mask]` will return all rows of df where mask is True. 

We can simplify this by putting the mask's logic statement into the df index directly, like `df[df.some_value > 5]`. 

To add complexity, we can filter based on multiple logic statements. 

```python
mask = ((df.value_a > 5) & (df.value_b == 11))

df[mask]
```

Note: Make sure to use `&` instead of `and`, and `|` instead of `or` for some reason when you do this. 

When we encounter **NA values**, we have to deal with them one way or another. But first, we have to find them. We do this by using `df.isna()` which will return a boolean indicating if each cell is an NA or not. To work with large dataframes, use `df.isna().sum()` to get a count of NAs per column. 

Now that we've found NA values, we either drop them using `df.dropna(subset=[])` where subset is a column or list of columns, or we can replace them with other values by using `df[some_column].fillna(value,inplace=True)`. 

What if we want to try to **apply a [[function]]** to the data or a subset of it? We can create a new column that is a function of other columns. We do this by using `df.apply(function)`, where `function` is whatever function you want applied, including a lambda function. This method will take a long time, but it works. For example, if we wanted to create a new column based on an input column, we could pass `df['new_column'] = df['input_column'].apply(function)'`. 

If we want a faster method, we can use `df.map()`. This method requires us creating a dictionary with the input column values as keys and the functions applied to those values as the paired values. 

**Grouping and aggregating data** is a powerful tool as well. We can sort the data by their score on a particular metric, like mean or maximum. This uses `df.groupby(column)` where we indicate the column we want the data grouped by. We could indicate multiple columns in a list to create a multilevel index. If we don't want a multilevel index when we do this, add `.reset_index()` to the end of the statement. 

If we want to include multiple metrics, we can use the `.agg()` method. By passing a dictionary to `.agg`, we can include multiple metrics and specify which columns we want each metric on. For example...
	
If we had a cereal nutrition dataset, we could group the data by manufacturer, and rank the data based on the number of cereals the mfr makes, the median calorie count, the mean sugars, and the mean rating:

```python
df.groupby('manufacturer')[['mfr','calories', 'sugars', 'rating']] \
    .agg({'mfr': 'count',
          'calories': 'median',
          'sugars': 'mean',
          'rating': 'mean'})
```

# Lecture - [[Git]] and GitHub
We will be using Git and [[GitHub]] for this bootcamp. Git has a steep learning curve, and we shouldn't worry if it's frustrating at first. 

The first thing to know is that Git is *not* GitHub. Git is a tool that provides version control. We can manage snapshots of our code, revert to older snapshots if something goes wrong, all while maintaining a clean environment without lots of copies of code, like `first_draft.py`, `final_draft.py`, and even `final_final_draft.py`, which is a sin I'm definitely guilty of. 

Turning to GitHub, this is a company that provides a nice UI and other tools built on git to help us out. 

## The git workflow
There are three stages in git: working directory, staging area, and repository. The working directory is the place where the currently existing code lives. The staging area is where we move code when preparing for a commit. We stage files for the commit and git checks to see if there is any changed content in the files. In this way, git can avoid doing pointless work later on. 

When staging files, we pass `git add <file>`, where `<file>` is the name of whatever file we want to stage. 

The third and final stage is the git repository. This is where all the snapshots, called checkpoints, live on our computer. A commit sends changes from the staging area into the repository. We pass `git commit -m "<comment>"`, where `<comment>` is a short but descriptive comment that we leave to let our future selves know why this commit was made, and what it contains. This command "commits" all our changes

## GitHub workflow
[[GitHub adds a fourth phase to Git]]. We create a *remote* repository that exists outside of our computer. This allows massive crowdsourcing of code. Libraries such as [[Scikit-Learn]] live as GitHub repositories that people can contribute to. 

There are a few commands that we will get to know when using GitHub. First of all, we can *push* our local changes to the remote repo to save them there. We can also *pull* changes that aren't yet reflected in our local code from the remote repo. 

There are some best practices when working in GitHub. We should commit often, keep seperate repositories for each of our projects, and document all commits with clear and concise messages. 

## The Metis workflow
For this course, we will be needing to *fork* and *clone* the `onl_ds5` repository. We also need to connect our local clone to the Metis version for updates to be delivered. 

```
git remote -v
git remote add upstream https://github.com/thisismetis/onl_ds5
```

Each morning, we should `cd` into our metis repository, and pass `git pull upstream main`. This will get the changes from the source repository and update our local repositories. If the remote repository has changes, and we've changed our local files, there will be a *merge conflict.* The best thing to do in order to avoid this is to not change any of our local files, and instead create a new repo if we want to work along with lecture. 

It's good to be familiar with markdown for constructing readmes. Check this article on [.md file on the Metis git workflow](https://github.com/thisismetis/onl_ds5/blob/main/curriculum/project-01/git-1/git.md) out. 

QUESTION: ask about GitHub Desktop and any issues with this shortcut
ANSWER: it's important to be familiar with [[command line]]. If we need to increase the complexity, we would have to use terminal anyway. 

## Diving into the Metis repository
First, we should inspect the README. A README is a file that the user is meant to read in order to gain context for the other files. The one for Metis has a ton of good stuff, and is basically the course homepage. We can sign up for appointments, check the schedule, look at the forms and documents that we will need, and submit deliverables. 

Projects and project starters, as well as investigations (10 min solo presentations later in the course), will be found and submitted through the README. We will also submit our blogs here once we come to that. 

We will take assessments, which are optional and really just used to see how we're doing. 

# [[MTA EDA]] intro
We should check out this [file](https://github.com/thisismetis/onl_ds5/blob/main/curriculum/project-01/project-01-introduction/project_01.md) to see the project intro. The prompt is to use MTA subway data to optimize the placement of signature-gatherers so that they get the maximum amount of foot traffic through stations. 

The data source is the MTA's internal accounting of turnstile usage in the entire system. The deliverable will be an 8-10 minute presentation and an organized project repository. 

For today, the goal is to meet with our group, clean the data, and check out the project starter. 

---
Next: [[1-05-21 metis]]