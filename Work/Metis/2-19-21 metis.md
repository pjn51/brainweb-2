# Metis Notes for Friday, Feb 19th
`LINKS:` [[metis week 7]]
#meeting/career

---
# Pair problem
Today we're working on an [[NMF]] problem. This was really confusing. 

- The sparse format means it has more zeroes than non-zero values
- We can store these kinds of matrices differently to save space if we just keep track of the coordinates of whatever cells aren't zero


# Lecture - [[recommendation]] 
The first ever product built around the idea of excellent recommendation was Pandora Radio. People went to Pandora *because* of their recommendation, which was a first. We'll get into how Pandora achieved this, but we first have to talk about what recommendation fundamentally is.

Broadly, given certain products or interests, we can recommend new things to a user based on their previous behavior. This is intuitive, and humans understand how to do this naturally. But we need to quantify these things so that a computer can do it for us. 

## Content-based filtering
We can use a *simple distance* method for recommendation. If we create scores on various attributes for movies, such as action-ness, comedy-ness, etc, we could calculate the [[euclidian distance]] between movies to give a good recommendation based on previous movies that the user liked. This requries a ton of domain knowledge, since we have to be able to accurately score all the movies. We also have to determine what features are really important. 

The first step here would be to create the features. Our mission is to be as objective as possible. Let's look at how Pandora did it. They hired teams of phd researchers to listen to music for hours on end. They created a *music genome project* to try and generate features. This was very labor intensive and expensive, but it paid off! This is called a *content-based filtering system.*

In some situations, we could have automatic content-based filtering, but it depends on the domain. Today, Spotify is really big on automatically creating features based on an audio profile. If we can vectorize the [[data]], we can work with it. 

The second step would be to scale and weigh the features. Scaling is important to normalize the features, and we can use domain knowledge to apply weights to make some features more or less important. 

The third step would be to calculate the distances, choose a recommendation based on the user. 

Unfortunately, when we have thousands and thousands of features, distance becomes more and more meaningless. In these situations, [[topic modeling]] becomes more useful. This has the added benefit of avoiding the recommendations being *too* good. If the recommendation is too similar, the result is boring. If a user who has watched *Spiderman* just gets *Spiderman 2* recommended, that was kind of a waste of time. The goal is to discover hidden connections. 

There are also other distance metrics we could use. We could use *cosine distance* which tends to do better than [[euclidian distance]]. 

## Collaborative filtering
But even more fundamentally, we could try and extract featuers in a totally different way. We could use the *users* to get the features. This is what YouTube does since there are waaaay too many videos to attempt to watch them and categorize them based on some number of features. Instead, they just look for patterns in user behavior. This is called *collaborative filtering.*

However, we have to be a bit careful. We can't treat a zero like a zero. Just because a user hasn't seen a video doesn't mean that they wouldn't like it if they saw it. 

## Combination techniques
Often, we can combine these two methods. We can begin with intrinsic knowledge of the things being recommended, since we don't start with user information. Then, we can transition towards collaborative filtering as we gain more and more user behavior data. 