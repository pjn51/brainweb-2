# Notes for Friday, January 15th
#meeting/career

---
Today is the last day of [[metis week 2]]. We are scheduled to start with some pair programming, and then we have a lecture on regularization and a regression assignment. I need to have collected my [[data]] for [[natural amenity regression]] by today. 

# Pair programming
The data for this problem are the numbers 2, 7, 1, 5, and 10.

We want to know which of the numbers from 0 to 10 (by tenths) gives the smallest result when we do the following:

For each of the numbers in the data, subtract the candidate number and then square the result, then add up these numbers.

For example, to test the candidate number 8.2, we do the following:

```python
2 - 8.2 = -6.2
         (-6.2)**2 = 38.44
7 - 8.2 = -1.2
         (-1.2)**2 = 1.44
1 - 8.2 = -7.2
         (-7.2)**2 = 51.84
5 - 8.2 = -3.2
         (-3.2)**2 = 10.24
10 - 8.2 = 1.8
           1.8**2  = 3.24

38.44 + 1.44 + 51.84 + 10.24 + 3.24 = 105.2
```

For candidate number 8.2, the result is 105.2. Can we get a smaller result for any of the other candidate numbers? Try them all and find the candidate number that gives the smallest result.

You might identify a shortcut to solving this problem. Write the code to try all the candidate numbers anyway.

Once you have an answer, also make a plot to show the results for all the candidate numbers.

As an extension, consider what class of problem this exercise represents. Can you write a general solution? Can you write a faster general solution?

## Basic solution
```python
import matplotlib.pyplot as plt

def pair():
    myrange = list(range(0,101))
    xlist = []
    ylist = []
    for candidate in myrange:
        candidate = candidate / 10
        
        cand_sum =  (2-candidate)**2 + 
					(7-candidate)**2 + 
					(1-candidate)**2 +
					(5-candidate)**2 +
					(10-candidate)**2
        
        xlist.append(candidate)
        ylist.append(cand_sum)
        
    plt.plot(xlist,ylist)
```

## Mathematical solution
The [[calculus]] approach would be to view the final answer (5) as the local minimum of the [[function]]. Mathematically, we're trying to minimize

$$
(2-x)^2 + (7-x)^2 + (1-x)^2+(5-x)^2+(10-x)^2
$$

We can take the derivative of this and set it equal to zero. 

$$
2(2-x)+2(7-x)+2(1-x)+2(5-x)+2(10-x)=0
$$

$$4-2x+14-2x+2-2x+10-2x+20-2x=0$$ $$50-10x=0$$$$x=5$$

This is the local minimum of the [[function]]. 

## Vectorized solution
Python is a lot slower than C++. We can use it for [[data science]] only by using things like Numpy. Numpy basically sends stuff to C++ code! This way we can enjoy the best of both worlds, because C++ is a pain in the ass to use. It doesn't do any of the background clean up that python does.

This is why [[loops]] in python are so much slower than numpy procedures are. We should stick to vector operations whenever possible, avoiding [[loops]], which are basically scalar operations. 

[[NumPy]] is faster, but only when using np arrays and matrices. Make sure to keep your data in this form. 

Here's a faster way to do the above code:
```python
data = np.array([2,7,1,5,10])
candidates = np.linspace(0,10,101)

def my_sum_of_squares(x, data):
	return sum((data-x)**2)

sum_of_squares = np.vectorize(my_sum_of_squares,excluded=[1])

results = sum_of_squares(candidates, data)

plt.plot(candidates, results)
```
`np.linspace` creates an array ranging from 0 to 10, with 101 data points between those markers. Therefore, it ends up having a step of 0.1. 

# Lecture: The [[bias-variance tradeoff]]
The bias-variance tradeoff is a fundamental concept when it comes to model tweaking and selection. Whenever we want to understand a new algorithm, think of its [[complexity]] and the position it holds in the bias-variance tradeoff. 

There are a few sources of model error. First, a model can be *biased.* This means that the model is missing *real patterns in the data.* This is caused by *underfitting.* A model can also have high *variance.* This means that the predictions that a model gives us fluctuate too much. As we increase the sensitivity of our model, we increase the variance as well. Fundamentally, variance is caused by a model *overfitting* to the [[data]]. There is also irreducible error, but we don't really have to worry about that. 

If we imagine the "real pattern" in the data as a bullseye, we would want all our predictions right in the middle of the bullseye every time. If a model has high variance, the predictions will be all over the place, evenly distributed around the bullseye. If a model has high bias, the predictions might all be near one another, but will be off to the side, not dead center. 

Sometimes, we have to make a choice on which form of error to eliminate. In that case, *we prefer high variance to high bias.* This is because we can get more data to reduce the variance. However, in cases where we cannot get any more observations to train our model, maybe a high-complexity mode is a bad option. In this case, maybe a high-bias model would be better. 

We can visualize the relationship between bias and variance as a parabola. As model complexity increases, total error first decreases, then reaches a minimum, and then begins to increase again. Our goal is to find the optimal middle point, where bias and variance are balanced. 

![chart](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/08/eba93f5a75070f0fbb9d86bec8a009e9.png)

# Regularization
To be specific, [[regularization]] is any step that we take to reduce *overfitting* of a model to the data on which it is trained. 

Regularization is very important because as we apply more and more complex and sensitive models to our data, we run the risk of our models hallucinating patterns where none exist. For example, if we were to apply a high-degree polynomial to a dataset of daily [[average]] temperatures, it might say that there is some unbelievably complicated relationship between the day of the week and the average temperature, which we know is not a real pattern. 

We apply regularization by adding a term to the [[loss function]] of our model. The basic loss function of a model is represented using some metric such as [[MSE]] or another way of measuring how well a model fit to the data. To this, we add a new term:

 $\lambda$ adjusts the intensity of regularization. By scaling $\lambda$ up, we can penalize the model more harshly for having lots of large coefficients, aka being more sensitive to the data. 
 
 A value of zero for $\lambda$ means that we don't care at all how sensitive the model is towards the data, and therefore during training we would end up choosing a model that seems to fit the training data *really* well, but will probably not generalize onto data the model has never seen. The model has overfit to the data. 
 
 As we increase $\lambda$ to a really high number, it eventually is the only consideration of our loss function. This means that the model would have no coefficients at all, and would just be reduced to an intercept. 
 
 $$M(x) = i + ax_1+bx_2+...+zx_n$$
 
 If this is our model, increasing $\lambda$ would push down the values of *a, b,* and *c* all the way to zero, leaving our model as $M(x)=i$. This would be equivalent to a horizontal line running through the cloud of datapoints, to use a 2D example. 
 
 So now that we know what $\lambda$ represents, what do we apply it to?
 
 When dealing with a [[linear regression]], we can do this in one of two similar ways. The first is [[ridge regression]] 

$$
C = \sum{MSE} + \lambda \sum{\beta^2}
$$

This loss function $C$ is saying that the loss of the model is the sum of all [[MSE]] across the dataset, plus $\lambda$ times another term, $\sum{\beta^2}$, where $\beta$ represents the size of each coefficient within the model. In Ridge regression, we square $\beta$. 

A slightly different way to do this is found in [[LASSO regression]]. Instead of squaring $\beta$, we take the absolute value of each coefficient. 

$$
C = \sum{MSE} + \lambda \sum{|\beta|}
$$

The difference between these two methods is subtle but important. When we increase $\lambda$, the model that has the lowest loss will have smaller and smaller coefficients on each variable. But with Ridge regression, these coefficients will never reach zero. With LASSO, they will. 

This means that LASSO actually drops terms from the model that have weak predictive power over the target feature. 

| Type  | Pro                                | Con                     |
| ----- | ---------------------------------- | ----------------------- |
| Ridge | Doesn't erase multicolinarity      | Never discards features |
| LASSO | Great for trimming excess features | May discard useful info | 

If we feel like the target feature is determined by lots of tiny effects from many features, we should go with a Ridge regression. But if the target feature is determined more by strong effects that we want to search for out of a haystack of features, LASSO is our best bet. 

## Coding application
See the `regularization_code.ipynb` in my `onl_ds5` repo. 

Standard import with some new toys.
```python
%pylab inline
#% config InlineBackend.figure_format = 'svg'

import pandas as pd
import [[seaborn]] as sns
sns.set()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV
from sklearn.metrics import r2_score
```

We will use the housing data from last time. See notes from [[1-14-21 metis|yesterday]] for more detail about this cell. 
```python
## Load in the Ames Housing Data
datafile = "data/Ames_Housing_Data.tsv"
df=pd.read_csv(datafile, sep='\t')

df = df.loc[df['Gr Liv Area'] <= 4000,:]

smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', 
                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', 
                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', 
                      'Garage Cars','SalePrice']]

smaller_df = smaller_df.fillna(0)
```

We still use a pair plot to see relationships.
```python
sns.pairplot(smaller_df)
```

Setting up for modeling. We separate features from the target, create a polynomial term for quality based on overall quality squared, and then perform a training-validation-test split on the data. 
```python
#Separate our features from our target
X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', 
                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', 
                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', 
                      'Garage Cars']]

y = smaller_df['SalePrice']

# create overall quality squared term, which we expect to 
# help based on the relationship we see in the pair plot 
X['OQ2'] = X['Overall Qual'] ** 2 



#Split the data 60 - 20 - 20 train/val/test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=43)
```

Now we can create a LASSO model using some selected columns. We instantiate the model, then we fit it to the training data. Then we look at a list of LASSO coefficients to see which ones LASSO made zero.
```python
selected_columns = ['Lot Area', 'Overall Qual', 'Overall Cond', 
                    'Year Built','Year Remod/Add', 'Gr Liv Area', 
                    'Full Bath', 'Bedroom AbvGr', 'Fireplaces', 
                    'Garage Cars', 'OQ2']

lasso_model = Lasso(alpha = 1000000) # this is a VERY HIGH regularization strength!, wouldn't usually be used
lasso_model.fit(X_train.loc[:,selected_columns], y_train)

list(zip(selected_columns, lasso_model.coef_))
```
We will see how LASSO sets some variables to zero when the alpha parameter is high. Alpha = $\lambda$. Actually, most of the variables were set to zero. The only ones that survived were the lot area and the ground living area. 

This is because we didn't standardize the scale of these features. Lot area survived because it was a lot bigger of a number than the number of bathrooms was, for example. We will do this later. 

LASSO also automatically takes care of ==colinearity==. When it notices that two columns are saying the same thing, it will zero out one of them. 

Let's see what Ridge does. We will set $\lambda$ really high (the alpha parameter)
```python
lr_model_ridge = Ridge(alpha = 1000000000000)
lr_model_ridge.fit(X_train_collinear, y_train)

list(zip(X_train_collinear.columns, lr_model_ridge.coef_))
```
Ridge will smooth out the coefficients, but won't bring them down to zero. Ridge also gave roughly equal weight to the two highly colinear features. We don't want to drop the highly colinear features since there might still be useful info despite the colinearity. 

Diagnostics on the LASSO model. We will plot the predicted values agains the values from the test set, and calculate r-squared and MAE. 
```python
test_set_pred = lasso_model.predict(X_test.loc[:,selected_columns])

plt.scatter(test_set_pred, y_test, alpha=.1)
plt.plot(np.linspace(0,600000,1000), np.linspace(0,600000,1000))

#r-squared
r2_score(y_test, test_set_pred)

#Mean Absolute Error (MAE)
def mae(y_true, y_pred):
    return np.mean(np.abs(y_pred - y_true)) 

mae(y_test, test_set_pred)
```
We can see that r2 is 0.52, and MAE is 39463, meaning that we are $39k off on average. Not so good if we need to be more specific than that, but better than any person could guess!

### Standard-scaling features
We really should have done this from the beginning. The sizes of our coefficients needs to be standardized for the loss function to work correctly. We will import some tools, apply them to the dataset, and then visualize the resulting scale of the features using a histogram.
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

## This step fits the Standard Scaler to the training data
## Essentially it finds the mean and [[standard deviation]] of each variable in the training set

std = StandardScaler()
std.fit(X_train.values)

## This step applies the scaler to the train set.
## It subtracts the mean it learned in the previous step and then divides by the standard deviation

X_tr = std.transform(X_train.values)

## Apply the scaler to the test set

X_te = std.transform(X_test.values)

## Note that even though we put a [[pandas]] Dataframe into the scalar, what comes out is a numpy array
## In general, sklearn works on numpy.  It will accept pandas objects by trying to coerce them to numpy arrays
## But it will not usually output pandas objects

type(X_train), type(X_tr)

## Here we can plot histograms of the transformed variables
## Note that they seem to have means of 0 and stddevs of 1
## (though they are not necessarily normally distributed)

plt.hist(X_tr[:,3])
```

Now that we have scaled our features, we can apply LASSO again. We instantiate a LASSo model, then fit it to the training data. Then let's look at a list of features with their new coefficients in LASSO. 
```python
## Fit a LASSO model on the standardized data

lasso_model = Lasso(alpha = 10000)
lasso_model.fit(X_tr,y_train)

## Note that now we can meaningful compare the importance of
## different features, since they're on the same scale

## But it's now difficult to interpret the coefficients
## We would need to translate back to the original feature scales by dividing
## each coefficient by the original column's standard deviation

list(zip(X_train.columns, lasso_model.coef_))
```
We can see that there are fewer zeroes than before. This means LASSO is taking more features into account. 

### Tuning regularization strength via validation
We will be selecting between different candidate models here. We fit a LASSO model, then [[model validation|validate]] it per value of alpha. 

Create a list of alphas, create arrays for storing errors against the training data and validation data per model. Then use a [[loops|for loop]] to loop through all the possible alpha values and run diagnostics on them to calculate their fitness against the training and validation datasets. 
```python
alphalist = 10**(np.linspace(-2,2,200))
err_vec_val = np.zeros(len(alphalist))
err_vec_train = np.zeros(len(alphalist))

for i,curr_alpha in enumerate(alphalist):

    # note the use of a new sklearn utility: Pipeline to pack
    # multiple modeling steps into one fitting process 
    steps = [('standardize', StandardScaler()), 
             ('lasso', Lasso(alpha = curr_alpha))]

    pipe = Pipeline(steps)
    pipe.fit(X_train.loc[:,selected_columns].values, y_train)
    
    val_set_pred = pipe.predict(X_val.loc[:,selected_columns].values)
    err_vec_val[i] = mae(y_val, val_set_pred)
	
#plot the curve of validation error as alpha changes
plt.plot(np.log10(alphalist), err_vec_val)

## This is the minimum error achieved on the validation set 
## across the different alpha values we tried
np.min(err_vec_val)

## This is the value of alpha that gave us the lowest error
alphalist[np.argmin(err_vec_val)]
```
We found the minimum error value was 22552, and the alpha used to get that was 20.72. So mathematically, that is our $\lambda$. `np.argmin()` returns the index for the minimum value in an array. 

### Automated regularization strength tuning via cross-validation
This is the better way to do it, but it's more computationally intensive. We will get a distribution of scores for each value of alpha, ensuring that the validation set was not an outlier in the data, messing with our choice of the best alpha (aka $\lambda$). LassoCV takes a vector of alphas, and does cross validation for each value of alpha, and gives us the value of alpha that has the smallest mean error. 
```python
## Scale the data as before
std = StandardScaler()
std.fit(X_train.values)

## Scale the Predictors on both the train and test set
X_tr = std.transform(X_train.values)
X_te = std.transform(X_test.values)

# Run the cross validation, find the best alpha, refit the model on all the data with that alpha
alphavec = 10**np.linspace(-2,2,200)

lasso_model = LassoCV(alphas = alphavec, cv=5)
lasso_model.fit(X_tr, y_train)

# This is the best alpha value it found - not far from the value
# selected using simple validation
lasso_model.alpha_

# These are the (standardized) coefficients found
# when it refit using that best alpha
list(zip(X_train.columns, lasso_model.coef_))

# Make predictions on the test set using the new model
test_set_pred = lasso_model.predict(X_te)

# Find the MAE and R^2 on the test set using this model
mae(y_test, test_set_pred)
r2_score(y_test, test_set_pred)
```
We can see that the r2 is a bit better than the other models we used, because the validation step was more robust. 

### Bonus: Using the LARS path to study feature importance
This plots the coefficients of the different features from most regularized to least regularized. We can see what happens to each coefficient as regularization decreases. 
```python
from sklearn.linear_model import lars_path

## Scale the variables
std = StandardScaler()
std.fit(X_train.values)

X_tr = std.transform(X_train.values)

## Note: lars_path takes numpy matrices, not pandas dataframes

print("Computing regularization path using the LARS ...")
alphas, _, coefs = lars_path(X_tr, y_train.values, method='lasso')

# plotting the LARS path

xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]

plt.figure(figsize=(10,10))
plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle='dashed')
plt.xlabel('|coef| / max|coef|')
plt.ylabel('Coefficients')
plt.title('LASSO Path')
plt.axis('tight')
plt.legend(X_train.columns)
plt.show()
```

