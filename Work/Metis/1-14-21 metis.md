# Notes for Thursday, January 14th
`LINKS:` [[metis week 2]]
`TAGS:` #meeting/career

---
We will start with pair programming, and then we have two lectures: one on [[cross validation]], and another on feature engineering. I have a career support meeting later today. 

# Pair programming
The questions today are about permutations, combinations, and [[probability]].  

==Question 1== - License plates for cabs in a city have exactly and only 6 upper case letters. How many unique plates can the city give out?

A- $26^6$ combinations (6 digits, 26 options per digit)

==Question 2== - I pull 5 cards from a deck of cards, how many unique hands are possible?

$$\frac{52*51*50*49*48}{5!}$$

==Question 3== - There are 30 stocks. A broker rates each [[stock]] as buy or sell. How many unique portfolio of 'buy' stocks are possible?

$$Q^{30!/n!*(30-n)}$$

Where Q is the number of stocks we want, and n is the number of recommended stocks.

==Question 4== - I roll a dice 6 times, what's the probability that each roll gives a unique number (meaning no number shows up twice)?

1/1 x 5/6 x 4/6 x 3/6 x 2/6 x 1/6 = P(unique every time)

==Question 5== - Six individuals are sitting in a circle. I have six cards, three red and three blue. If I randomly hand each one a card, what is the probability that the colors would perfectly alternate, that is no two adjacent people have the same color?

6/6 * 3/5 * 2/4 * 2/3 * 1/2 * 1/1

==Question 6== - There are 10 kids who what to order ice cream. Each one can choose between chocolate, vanilla or strawberry. You'll collect their choices and make one order, something like: 5 chocolate, 3 vanilla and 2 strawberry. How many possible orders can you make, where every flavor is represented (8 chocolate, 1 vanilla and 1 strawberry is valid but 8 chocolate, 2 vanilla and 0 strawberry is not valid)?

8 C: only 1 V option, S has 10 - C - V options
7 C: 2 V
6 C: 3 V
5 C: 4 V
4 C: 5 V
3 C: 6 V
2 C: 7 V
1 C: 8 V

N * (N+1)/2
8 * 9/2 = 36 options

Combinatorix solution:
10 kids. We have to slice the group twice to make three groups. That means there are 9 places to cut, we choose 2 at random. If we do all possible breaks, we will arrive at all possible orders. 

# Lecture - [[model validation]]
Model validation is all about selecting the right model and estimating its quality. Once we have a model, we have to determine how "good" it is. This is essential for the workflow step: [[modeling]]

[[Machine learning is the process of developing abstractions]]. In brief, it's important to estimate the *generalization error* of a model. Does the model do well when used on new [[data]]? We can use validation and cross-validation strategies to determine this. However, we should remember that vaidation is not the same as testing.

To start, let's talk about testing. Predictive models are only useful if they can *generalize.* In order for a model to generalize, it has to take the pattern that it sees in the testing data and apply its knowledge to new information. We call this sort of evaluation *testing.* 

When we test a model, we want to simulate generalization. In order to do this, we use what is called a *train-test split.* The idea is to hold some of our data back from the model as a test set, and then see how well the model does on this test set after it is trained on the rest of the data (the training set). We only really care about what the error is on the testing set. This will almost always be higher than the error on the training set. 

Complex models may be able to fit *too well* to the training set. These models may hallucinate patterns in the data that don't actually generalize. This results in much lower scores on the testing set. In general, the more complex a model is, the more we have to worry about this.

## Validation
Validation is another important part of the process of evaluation. The train-test split method only lets us try one attempt at creating a good model. If we try again and again, seeking to improve our score on the test set, we might as well just fit right to the test set! We will run into problems generalizing. 

However, there are lots of reasons that we want to be able to try multiple times to model a dataset. There are lots of [[hyperparameters]] or pre-processing steps that we might want to tweak. In order to get around this issue, we turn to validation. 

We can add a validation step between train and test. Now we have a train-validate-test split. For example, we might split the [[data]] into 60% train, 20% validation, and 20% test. We train all the candidate models on the training set, score the candidates on the validation set, and then choose a single model that does best. This final model is then trained on the train + validation set, and scored against the test set. 

We should remember that validation is *not* testing. We use it to select the model, not to simulate generalization. We should also be sure to compare candidate models using the same metrics and validation scheme. Validation is also likely unnecessary when dealing with [[linear regression]] since the models rarely overfit. 

## Cross-Validation
Sometimes we could get unlucky during validation. What if we happen to overfit to the sample of the data that happened to be pulled for validation? We can use more of our data for validation than just 20% or so, through a method called *cross-validation.*

If we use a strategy called *k-fold partitioning,* we randomly divide our non-test data into *k* equal-sized groups, or folds. Each fold is used as a validation set once, and we will compare candidates via a mean score across all folds. We often use 5 or 10 folds. 

The workflow begins with dividing the data into test and train. Then, we split the data into *k* equal-sized groups. One at a time, each group acts as the validation fold, as the candidate model is trained on the rest of the folds. As we cycle through each fold's role as the validation fold, we keep track of the scores of each model run. At the end, we find the mean error score across all the folds. This provides a more robust representation of how well the model is really doing. 

## Recap
The simplest method is train-test split. Simple validation uses train-validate-test split. We can use cross-validation to make this step even more robust. 

With very large datasets, simple validation is generally good enough. We should use cross-validation when the dataset is small, or the variance in model performance is high. 

## Coding application
See the `validation_workflow_and_utilities.ipynb` notebook. 

We will be working with a cars dataset here, focusing on the numerical columns.
```python
import pandas as pd
import numpy as np

df=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header=None)

columns= ['symboling','normalized-losses','make','fuel-type',
          'aspiration','num-of-doors','body-style','drive-wheels',
          'engine-location','wheel-base','length','width','height',
          'curb-weight','engine-type','num-of-cylinders','engine-size',
          'fuel-system','bore','stroke','compression-ratio','horsepower',
          'peak-rpm','city-mpg','highway-mpg','price']
df.columns=columns
```

Cleaning the data.
```python
#simple cleaning
df = df.replace('?', np.NaN).dropna().reset_index(drop=True)
df['price'] = df['price'].astype(float)

cars = df.select_dtypes(exclude=['object']).copy()

# cars['make'] = df['make']
cars.head(3)
```

### Using simple validation
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder

# separate features from the target
X, y = cars.drop('price',axis=1), cars['price']

# hold out 20% of the data for final testing
X_, X_test, y_, y_test = train_test_split(X, y, test_size=.2, random_state=10)

# split remaining data for test-validation
X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=.25, random_state=3)
```

Now we create the candidate models, some scaled features, polynomial features. Then we fit the candidates to the training set and get their R^2 (see [[coefficient of determination]]) values against the validation set. 

```python
lm = LinearRegression()

#Feature scaling for train, val, and test
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train.values)
X_val_scaled = scaler.transform(X_val.values)
X_test_scaled = scaler.transform(X_test.values)

#Feature transforms for train, val, and test so that we can run our poly model on each
poly = PolynomialFeatures(degree=2)

X_train_poly = poly.fit_transform(X_train.values)
X_val_poly = poly.transform(X_val.values)
X_test_poly = poly.transform(X_test.values)

lm_poly = LinearRegression()

#validate

lm.fit(X_train, y_train)
print(f'Linear Regression val R^2: {lm.score(X_val, y_val):.3f}')

lm_poly.fit(X_train_poly, y_train)
print(f'Degree 2 polynomial regression val R^2: {lm_poly.score(X_val_poly, y_val):.3f}')
```

This will give us our R^2 value for the model using the validation set. For LinReg, its 0.35, PolyReg is -2.099!

Now that we can compare R^2 values between models, we can choose linear regression as the best model, and train the model on the train+validate data. 

```python
lm.fit(X_,y_)
print(f'Linear Regression test R^2: {lm.score(X_test, y_test):.3f}')
```

This is pretty good, with a R^2 of 0.858! Even better than with the validation set. That's cool. But why was the validation score low? It was just because of variance within the validation set. We randomly selected a pretty difficult part of the data to understand for validation, making the R^2 of our model low for the validation step. That's a good argument for cross-validation!

### Using cross-validation
We will break the data into 80% for cross-validation and 20% for final testing. Here, we import the KFold module and split the data up. 

```python
from sklearn.model_selection import KFold

X, y = cars.drop('price',axis=1), cars['price']

X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=10) #hold out 20% of the data for final testing

#this helps with the way kf will generate indices below
X, y = np.array(X), np.array(y)
```

Now we create the K-fold, indicating the number of splits. Then we create a place to collect the scores, and run a for-loop that [[loops]] through all the folds. This is all for validating our linear regression.

```python
#run the CV

kf = KFold(n_splits=5, shuffle=True, random_state = 71)
cv_lm_r2s = [] #collect the validation results

for train_ind, val_ind in kf.split(X,y):
    
    X_train, y_train = X[train_ind], y[train_ind]
    X_val, y_val = X[val_ind], y[val_ind] 
    
    #simple linear regression
    lm = LinearRegression()

    lm.fit(X_train, y_train)
    cv_lm_r2s.append(round(lm.score(X_val, y_val), 3))

print('Simple regression scores: ', cv_lm_r2s, '\n')

print(f'Simple mean cv r^2: {np.mean(cv_lm_r2s):.3f} +- {np.std(cv_lm_r2s):.3f}')
```

Now we do the same procedure for the polynomial regression, keeping track of all the scores, using the same number of splits, and the same random_state, meaning that all the folds will be identical between these two blocks.

```python
kf = KFold(n_splits=5, shuffle=True, random_state = 71)
cv_lm_poly_r2s = []

for train_ind, val_ind in kf.split(X,y):    
    #poly with degree 2
    poly = PolynomialFeatures(degree=2)

    X_train_poly = poly.fit_transform(X_train)
    X_val_poly = poly.transform(X_val)

    lm_poly = LinearRegression()
    
    lm_poly.fit(X_train_poly, y_train)
    cv_lm_poly_r2s.append(round(lm_poly.score(X_val_poly, y_val), 3))
    
print('Poly scores: ', cv_lm_poly_r2s, '\n')

print(f'Poly mean cv r^2: {np.mean(cv_lm_poly_r2s):.3f} +- {np.std(cv_lm_poly_r2s):.3f}')
```

### Shortcut using [[Scikit-Learn]]
We can shortcut around the manual way to do this above, and use the package instead. Be careful, because it take a lot of the control out of your hands. It limits the way that we can do pre-processing.

```python
from sklearn.model_selection import cross_val_score

# creating models for comparison
lm = LinearRegression()
poly = PolynomialFeatures(degree=2)

lm_score = cross_val_score(lm, X, y, # estimator, features, target
                cv=5, # number of folds 
                scoring='r2') # scoring metric

poly_score = cross_val_score(poly, X, y
				cv=5,
				scoring='r2')
```

The `cross_val_score()` function returns an array of all the r^2 scores for the folds. 

# Lecture - [[feature engineering]] with linear regression
We will be using the Ames housing dataset, with some recommended outlier cleanup. 

```python
import pandas as pd
import [[seaborn]] as sns
sns.set()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

## Load in the Ames Housing Data
datafile = "data/Ames_Housing_Data.tsv"
df=pd.read_csv(datafile, sep='\t')

# This is recommended by the data set author to remove a few outliers
df = df.loc[df['Gr Liv Area'] <= 4000,:]
```

There are a lot of variables, we will only look at a few numerical ones. We will also fill some NAs that are in the garage cars column.

```python
smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', 
                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', 
                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', 
                      'Garage Cars','SalePrice']]

# There appears to be one NA in Garage Cars - fill with 0
smaller_df = smaller_df.fillna(0)
```

We can visualize this dataset using a pair plot. This shows a TON of information and is hard to really see what's going on. It plots every column against every other column. But we can get insights from this plot. We can see relationships between features. For example, we can see a skew in our target, house price. We can also look and see what features are correlated with our target. We can also check for ==colinearity==. 

```python
sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))
```

Now we build a baseline model. First, we separate the features from our target (sale price). The `split_and_validate()` function creates a train-validate split, and reports the validation R^2 result for a linear regression.

```python
#Separate our features from our target
X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', 
                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', 
                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', 
                      'Garage Cars']]

y = smaller_df['SalePrice']

def split_and_validate(X, y):
    '''
    For a set of features and target X, y, perform a 80/20 train/val split, 
    fit and validate a linear regression model, and report results
    '''
    
    # perform train/val split
    X_train, X_val, y_train, y_val = \
        train_test_split(X, y, test_size=0.2, random_state=42)
    
    # fit linear regression to training data
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    
    # score fit model on validation data
    val_score = lr_model.score(X_val, y_val)
    
    # report results
    print('\nValidation R^2 score was:', val_score)
    print('Feature coefficient results: \n')
    for feature, coef in zip(X.columns, lr_model.coef_):
        print(feature, ':', f'{coef:.2f}') 
```

```python
split_and_validate(X,y)
```

We see a R^2 score of 0.81, which is good. 

## Basic feature engineering: adding polynomial terms
Whenever we see curved lines in the relationships between a feature and our target, we might want to add polynomial terms to the model. Here we add two quadratic polynomial terms to the model and see what the score is. 

```python
X2 = X.copy()

X2['OQ2'] = X2['Overall Qual'] ** 2
X2['GLA2'] = X2['Gr Liv Area'] ** 2

split_and_validate(X2, y)
```

Adding these terms allowed our model to do significantly better, about 0.05 better R^2

## Adding interaction terms
It's hard to see these in a pair plot. Sometimes different features together tell us more than the seperate effects of each feature. We can add terms that are functions of the interaction between multiple features. We find these kinds of things with our domain knowledge, not the pair plot. 

```python
X3 = X2.copy()

# multiplicative interaction
X3['OQ_x_YB'] = X3['Overall Qual'] * X3['Year Built']

# division interaction
X3['OQ_/_LA'] = X3['Overall Qual'] / X3['Lot Area']

split_and_validate(X3, y)
```

This boosts our model's R^2 by 0.01. 

Feature enginnering is an art, and we have to decide wether adding these kinds of things are worth it in the end, depending on the purpose of the model. 

## Categories and features derived from category aggregates
Let's incorporate categorical features, like house style. We have to get dummy variables using the built-in [[Pandas]] method, `get_dummies`. 

```python
X4 = X3.copy()

X4['House Style'] = df['House Style']

split_and_validate(pd.get_dummies(X4), y)
```

This gains us another increase for our R^2 by about 0.01. 

If we bring in another category, neighborhood, let's see what happens. But a lot of neighborhoods have only one value, so lets put the neighborhoods with less than 8 observations into a column called "other". 

```python
nbh_counts = df.Neighborhood.value_counts()

other_nbhs = list(nbh_counts[nbh_counts <= 8].index)

X5 = X4.copy()

X5['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')

split_and_validate(pd.get_dummies(X5), y)
```

This boosts R^2 by 0.01 again. But we add a lot of feature columns as a cost. 

As a final note, what if we want to add a more complex type of feature? We will need a function to add this feature, and then we can apply the function to another copy of our dataset. 

This function will create features that capture where a feature lies relative to the members of a category that it belongs to. 

```python
def add_deviation_feature(X, feature, category):
    
    # temp groupby object
    category_gb = X.groupby(category)[feature]
    
    # create columns of category means and standard deviations
    category_mean = category_gb.transform(lambda x: x.mean())
    category_std = category_gb.transform(lambda x: x.std())
    
    # compute stds from category mean for each feature value,
    # add to X as new feature
    deviation_feature = (X[feature] - category_mean) / category_std 
    X[feature + '_Dev_' + category] = deviation_feature  
	
X6 = X5.copy()

add_deviation_feature(X6, 'Year Built', 'House Style')
add_deviation_feature(X6, 'Overall Qual', 'Neighborhood')

split_and_validate(pd.get_dummies(X6), y)
```

This just helped our model a tiny bit, but it's cool so we'll keep it. 