# Metis Notes for Friday, Feb 12th
`LINKS:` [[metis week 6]]
#meeting/career

---
Today we're starting with a pair programming exercise, and then we have lectures on SVD and [[PCA]] later on. 

## Pair exercise
We're creating a simulation to perform a *Monte-Carlo* simulation in n-dimensions.

First, let's just try it in two dimensions. We want to create a square, and an inscribed circle within it. We will randomly sample points within this square, and find what % fall within the circle, coming up with a ratio.
 
```python
def eu_dist(x,y):
	return np.sqrt((x**2)+(y**2)) < 1: # 1 is the radius of the unit circle

x_list = []
y_list = []

for i in range(1000):
	x_list.append(np.random.uniform(-1,1))
	y_list.append(np.random.uniform(-1,1))
	
inside = 0
outside = 0
for i in range(1000):
	if eu_dist > 1:
		outside += 1	
	else:
		inside += 1
		
print(inside / (inside+outside))
	# returns a ratio of in vs out
```

Now let's do it in three dimensions.
```python
def eu_dist_3d(x,y,z):
	return np.sqrt((x**2)+(y**2)+(z**2)) # 1 is the radius of the unit circle

x_list = []
y_list = []
z_list = []

for i in range(1000):
	x_list.append(np.random.uniform(-1,1))
	y_list.append(np.random.uniform(-1,1))
	z_list.append(np.random.uniform(-1,1))
	
inside = 0
outside = 0
for i in range(1000):
	if eu_dist > 1:
		outside += 1	
	else:
		inside += 1
		
print(inside / (inside+outside))
	# returns a ratio of in vs out
```

The point of this exercise is to see that when we add dimensions, we run into a problem. It adds more and more noise to the signal when we're looking for patterns. This is called *the curse of dimensionality*.

On average, points are farther apart when there are more dimensions to work with. When this happens, the distance between points that are related and points that aren't related becomes more and more similar, and harder to distinguish between. 

This is relevant to us right now since when we work with [[NLP]], we're gonna have a *lot* of features, and therefore a *lot* of dimensions.  

In fact, lots of models struggle with this curse, because any model that cares at all about distance will be trapped. This leaves only decision trees exempt from the curse. (also naive bayes does well with high dimensionality). 

## Lecture - the curse of dimensionality
[[Beware the curse of dimensionality]]!
- Training [[accuracy]] as we add dimensions
	- With a single dimension, our training accuracy will be low-ish. Close to zero. 
	- If we have like a thousand dimensions, the training accuracy will be 100%. 
	- This is because we're talking about *training accuracy*. The more features we have, the easier it is to overfit. 
- Does adding data increase performance?
	- More data is not always better
	- Especially if we just add dimensions without adding observations
	- After 10 features or so, we need to start thinking about the curse of dimensionality. 
	- ==Colinearity== is another reason we should hesitate before adding new features willy nilly. 
	- Adding features also decreases our ability to interpret the results. 
- Ratio of observations to features
	- When we have too few observations, we run into more problems. 
	- This is really the issue with adding more features without adding more observations. 
	- When your features are potentially correlated, the ideal number of features to have is the square root of the number of observations, *n*, that your data have. $$\sqrt{n}$$This is the turning point where adding new features no longer help.
	- Note that this is a theoretical quantity, and real world data is often different. When we're really working with data, just make sure you're looking for the potential problems that might be happening when adding lots of features. We don't have to stick to this guideline all the time. 
- Sparsity increases as dimensionality increases. 
	- However, a single dimension isn't so good either since there isn't much *seperability* between points. 
	- But too much seperability is also a bad thing. Usually we are dealing with too much seperability. 
- Reducing dimensionality
	- If we want to fight back against the curse, we can try and reduce the number of dimensions without getting rid of too much useful information. 
	- We could also try out a decision-tree model that works better with higher dimensions. 

## Lecture - SVD and [[PCA]]
*SVD* stands for Singular Value Decomposition, while *PCA* stands for Principal Component Analysis. They are related but technically different. PCA is the swiss army knife of [[dimension reduction]], while SVD is the engine that makes the PCA car work.

First, we're going to cover the assumptions of [[dimension reduction]], and then we will cover matrix decomposition using SVD, then PCA, and then the assumptions that PCA makes.

### Assumptions of dimension reduction
In order to reduce the dimensionality of our data, we have to assume that the patterns in our data mostly lie in a lower dimension. If this is false, then reducing the number of dimensions is going to erase the patterns in the data. 

We have to find a new axis to perform dimension reduction, since we have to re-orient the coodinate space.

### SVD for decomposition
Matrix decomposition is a broad term that has lots of techniques. We are basically trying to take one matrix, and rewrite it as the product of several matrices. 

We have to differentiate between feature *selection* and feature *extraction*. Feature extraction creates artificial new features that are composites of pre-existing features. Feature selection is more of an art. When we select features, we are making a claim about which characteristics of the observations are useful. 

Feature extraction is a lot easier, but we need to be careful that we're applying it in a beneficial way for our use case. 

### PCA overview
PCA is an [[unsupervised learning]] method. We don't give PCA any labels to work with, only the input features. Each principal component is a linear combination of all features, meaning that it's a linear combination of all dimensions that the data exist in. We assign a weight to each of these features, called a *loading.* 

Make sure to standardize data before PCA!

There are some drawbacks to applying PCA. We lose the ability to really interpret what these pricipal components *mean.* We move from named featuers, like height, weight, and age, to PCs that are just "PC1," "PC2," etc.

### Applications of dimension reduction
We can use this for [[NLP]], specifically [[topic modeling]]. We have our axes as topics, and documents can cluster around these axes. We use PCA to specify the axes. 

Dimension reduction can be useful to visualize >3D feature spaces. It also *sort of* reduces computational [[complexity]]. We still have to figure out the principal components, but down the road, we have fewer features to work with which can be nice.

Dimension reduction is useful for image and audio compression, since it allows for data compression with minimal loss in information. 