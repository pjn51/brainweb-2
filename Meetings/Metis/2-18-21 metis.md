# Metis Notes for Thursday, Feb 18th
`TAGS:` #meeting/career

---
Today is the start of the end of [[metis week 7]]. We're starting with a pair related to [[LDA]], and then we have a lecture on *embedding*, whatever that is. 

# Pair problem
We will start with a probability distribution of words within a topic, and we're going to generate documents that fit that probability distribution. This is sort of the reverse process that we will be doing for our project. 

For example, if we want to create a document that is 50% topic A and 50% topic B, if the length is 100 words, we would choose 50 from the probability distribution of words within topic A, and 50 from the distribution found across topic B. 

You are given documents as probability [[distributions]] over topics, and topics as probability distributions over words.

Implement a function `make_doc` that takes a document (as `topic_probs`) and a number of words. The function should randomly generate a document by choosing a topic for each word using the document's topic probabilities and then choosing a particular word using that topic's word probabilities. The function should return a string containing all the generated document's words.

For example:

```python
docs = 	[0.98, 0.01, 0.01]

topics = [0.4, 0.4, 0.01, 0.01, 0.01, 0.01, 0.1, 0.04, 0.01, 0.01]

words =  ['cat', 
		  'kitten', 
		  'dog', 
		  'puppy',  
		  'deep', 
		  'learning',
          'fur',  
		  'image',  
		  'GPU', 
		  'asparagus']

for doc in docs:
	print make_doc(topic_probs=doc, n_words=10)

#  Example output:
## cat learning kitten image kitten cat deep image cat kitten
## puppy puppy learning dog puppy dog dog puppy image dog
## deep learning deep image deep deep deep deep learning learning
```

Our solution:
```python
def make_doc(topic_probs, n_words):
	doc = ''
	for i in range(n_words):
		nextword = np.random.choice(words, topics)
		doc = doc + ' ' + nextword

```

Brian's solution
```python
def make_doc(topic_probs=None, n_words=10, verbose=True):
	if topic_probs is None:
		topic_probs = np.random.dirichlet(np.ones(len(topics)))
		
	if verbose:
		print('topic_probs:', topic_probs)
		
	results = []
	for i in range(n_words):
		i_topic = np.random.choice(len(topics), p=topic_probs)
		topic = topic[i_topic]
		word = np.random.choice(words, p=topic)
		results.append(word)
	
	return ' '.join(results)
```

The general idea here is to reverse the process of topic modeling that we will be doing for our projects. 

# Lecture - [[embedding]]
This lecture will function as an introduction to *word embeddings.* When we want to allow a computer to understand written words, we must translate the words into a language that computers can understand. This means we have to translate these words into numbers that we can perform operations on. 

## Count vectorization
The simplest possible method of embedding is the count vectorizer. We just ask if the word exists in the document, or we might ask how many times it appears. We can use this information to create a *document-term matrix,* where each unique word has become its own feature. This matrix has the shape $N \cdot V$, where $N$ is the number of documents in the corpus, and $V$ is the size of the vocabulary of the corpus. 

There are some drawbacks to this approach. We lose the order of words, which means we lose a ton of context. For example, if a document says "This movie is not good," we lose the fact that "good" has been negated and merely record that the word "good" appears. 

## TF-IDF
The next level of complexity is [[TF-IDF]]. This stands for "term frequency - inverse document frequency." The first part is "term frequency." We use term frequency instead of raw counts so that we can take into account how common each word is *over the entire corpus.* This helps us gauge how important a word is to the meaning of one individual document in which it appears. For example, the word "and" will likely appear in most documents, so we can discount its importance in determining meaning. 

By taking frequeny into account, we can reduce the noise created by words that appear all over the place but reveal little meaning. 

The second part of this formula is *inverse document frequency.* 

Mathematically, TFIDF is defined as follows:

$$
\text{TFIDF} = \text{TF} \cdot \text{IDF}
$$

This method is still based on a bag-of-words approach where we lose context, but it's better than a simple count vectorizer. 

## Word2Vec
So far we've been dealing with *sparse matrices* which aren't very efficient. We've also been losing semantic and contextual information. Word2Vec allows us to avoid both these problems. 

We can leverage the *distributional hypothesis,* which proposes that words that are used in similar contexts tend to have similar meanings. Therefore, we can approximate the meaning of words based on their surrounding words. 

We can use this information to model the probabiities of *sequences* of words instead of just looking at every word in isolation. 

If I want more information, I should check out this [tutorial](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1). 

### How it actually works
We could use a *unigram* model, where we look at the probability of a single word appearing in isolation. We could move towards a *bi-gram* model, where we look at the probability of a two-word sequence appearing. Word2Vec actually uses a *skip-gram* model, where the algorithm looks at the probability of a word appearing, and the probabilities of what words will appear just after the word, two words later, and so on (in both directions).

These probabilities are calculated using two vectors per word. We use one vector where the word in question is the central word, and another vector where the word in question is used as a context word. To this, we apply a [[loss function]] and minimize. 

### Results of Word2Vec
This algorithm can capture interesting connections and concepts, such as finding relationships between a city and the country the city is found in, as well as other interesting relationships. Interestingly, if we add vectors together, we can find combinations of concepts and ideas! If we add "Washington" and "mountain," we may get Ranier. These results are arrived at by training general langauge models for weeks or even months at a time on staggeringly huge datasets. 

### Implementing Word2Vec
Google provides pre-trained vectors that we can utilize without having to train our own models, although we can do that if we want to.

## GloVe
This algorithm is a global context model, sort of like Word2Vec. While Word2Vec looks at the frequency of words appearing locally with one another, gloVe looks at the frequency of words appearing in the document with one another more generally. This is a very high-dimensional method, since we basically create a count vectorizer that reports the frequency of every possible combination of words in documents. 

The real meaning comes from the ration of co-occurence. Words that tend to have co-occurence with all other words cancel themselves out, while groups of words that often appear together can be discovered. 

### Technicalities of gloVe
We begin by fitting the gloVe vectors to the co-occurence probabilities using a *log bilinear* model. Then we can use center and context vectors in the same way that Word2Vec does. We examine the relative differences between scores rather than the scores themselves. GloVe also uses dynamic stop words to remove words that are really common across all documents. 

### GloVe's loss function
We can treat this modeling task as a weighted least-squares problem.

### GloVe results
We can find the [[euclidian distance]] between two word vectors to determine the linguistic similarity between two words.

## Working with word embeddings
First we should start with a count vectorizer. Then we can move on to Word2Vec and maybe gloVe to see how they compare. We should probably just use Google's Word2Vec model rather than training our own since that takes a lot of time and [[data]]. 

## Beyond gloVe and Word2Vec
These models are getting a little bit old. There are some [[deep learning]] models that show promise creating contextualized word embeddings to deal with words that have multiple meanings depending on context. 
