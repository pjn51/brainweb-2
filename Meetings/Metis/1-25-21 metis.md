 # Metis Notes for Monday, Jan 25th
 `TAGS:` #meeting/career
 
---
Today is the beginning of [[metis week 4]]. We're starting off with part 2 of the regression assesment, followed by lectures on KNN classification, [[SQL]], and an introduction to our third project. 

## Ridge family time
Let's talk about our [[data science workflow]] we've used so far. First, we have to decide on a topic. We brainstorm meaningful questions, and examine available [[data]]. 

Then, we can scrape the data or otherwise get it. This often involves finding multiple data sources and merging them. 

After that comes [[exploratory data analysis]]. We can look for potential errors or outliers in the data through [[summary stats]]. We can also examime general potential for analysis in the target and the features. This step helps us get hints as to how we can approach the data best.

Finally, we can move onto the most expansive step, [[modeling]]. We should always seek to create a MVP (minimum viable product) and iterate on that. We should evaluate using some kind of model error, taking into account the [[bias-variance tradeoff]]. Note that these metrics need to be intrepreted and applied to the use-case for the model. Don't just report th e [[coefficient of determination]], report how well the model will actually work for the uses that your team needs it for.

Once we've created a model, we should optimize model [[hyperparameters]] and perform model selection using cross validation. 

- Decide on a topic
	- Brainstorm meaningful questions
	- Examine available data
- Scrape data
	- Find different data sources
	- Merge data sources
- EDA
	- Look for potential errors or outliers in the data through summary stats
	- Examine potential for analysis in the target and features
	- Get hints about how to best model the data
- Modeling
	- Create a MVP, a simple model to see what to do next.
		- Evaluate using some metric of model error, taking into account the [[bias-variance tradeoff]]. Note that these metrics need to be *interpreted* and applied to the use-case for the model. Don't just report the R2, hreport how well the model will actually work for the uses that your team needs it for. 
	- Optimize model hyperparameters and perform model selection through validation
		- We pick the best model and the best regularization strength for that model on the validation dataset so that we don't overfit to the test data. 
		- To be efficient with our data and prevent outliers from messing with our validation, we can use ==cross-validation== on multiple folds of the training-validation data. 
			- For each model, we will perform 5 train-[[model validation|validate]] splits (assuming we are using a K-Fold method with 5 folds). We take the average error for each model and compare them. 

General wokflow ideas:
- Workflows aren't waterfalls. We will bounce around as we get new information. 

For more on this concept, see my note on the [[data science workflow]]. 

# Lecture: Intro to [[classification]]
To review, supervised learning is when we have labelled training data. We fit a model to this data, and try and make predictions about unlabeled data. 

[[regression]] seek to predict a numeric target. Fundamentally, [[classification]] seek to predict labels. For example, Gmail tries to predict if a new email is spam or not. 

The workflow for classification is similar to that of a regression project, including model selection, tuning, and validation. We need to be specific in what kind of question we ask. A good regression question would ask how long it will take for a specific dog to get adopted. A classification question would ask if that dog will be adopted within a given timeframe. Similar problems can be approached differently.

There are many algorithms that we could use to try and make a prediction. We could use [[KNN]], [[logistic regression]], [[naive bayes]], or [[CART models]]. 

# Lecture: K-nearest neighbors algorithm
There's a saying that you can tell a lot about a person based on the company they keep. At least that's the opinion of the [[KNN]] algorithm. 

A broad overview of the KNN algorithm is that we choose a value for *k* and a distance metric, usually [[euclidian distance]]. Then, we find the *k* nearest neighbors to the point that we want to classify. Finally, we assign the class label to the unknown point by looking at which class is most common in these neighbors. 

We can think of the feature space as an area of multiple *decision regions* when we use KNN, as well as many other [[classification]]. Based on what decision region a new point falls into, we assign a class to it. These decision regions are divided by *decision boundaries,* where points sit on the fence between multiple classes. 

Every algorithm has different kinds of decision boundaries that they can handle. KNN is pretty flexible, and allows for enclave regions with a low enough value of *k.* As we increase *k,* we smooth out the decision boundaries, since we start to incorporate information from larger and larger areas. At a large enough value for *k,* the whole feature space would consist of a single decision region. 


![decision regions](https://miro.medium.com/max/1264/1*4vdvnkZoWgOp0vcLF4wFcA.png)


KNN is good at working with more than two classes. It can really break the data into however many classes are found in the training data. 

If *k* is too small, we run the risk of overfitting. But if *k* is too large, we may underfit. We also need to remember to *scale the data when using KNN.* If two features have different numeric scale, such as height and weight, then KNN will think that the distance between datapoints is more significant just because of the larger scale of the data.

| Pro                              | Con                                                 |
| -------------------------------- | --------------------------------------------------- |
| Easy to implement                | Lazy (no training time, just memorizes data)        |
| Interpretable process            | Memory intensive                                    |
| Adapts to new training data well | Prediction time scales linearly w new training data |

                          
# Lecture: [[RDBMS]] and [[SQL]]
For more detail, see the `02_SQL_Exercises.ipynb` notebook
- Why use a [[database]] / SQL?
	- Keeps everything up to date
	- Don't have to pull all the data for every analysis (saves space)
- RDBMS
	- Relational Database Management Systems
	- Contain structured data
		- Tables that relate to one another
		- Predetermined data types, fields, etc
	- [[postgres]] is a RDBMS
- [[SQL]] basics
	- CRUD
	- Today we'll mostly be reading from databases
- [[SQL]] order of execution
	- SQL has lots of commands we can execute.
	- The order in which we write commands is *not* the order in which they're executed. 
- `SELECT`
	- We put this at the start of a lot of commands, it prepares SQL to select data. 
	- We can say `SELECT *` to pull *all* the records. 
- `FROM`
	- This tells SQL where you want the data to come from.
- `WHERE`
	- This is where we can put criteria for what we want to select.
	- We can chain criteria together using `AND`

```SQL
SELECT * FROM mydatabase WHERE yr = 2010 AND day = 'Monday';
```

- `GROUP BY` 
	- We can group our data by a specific feature using this command. 


```sql
SELECT state, sum(freq) FROM mydatabase 
WHERE (name = 'John' OR name = 'Johnny')
AND yr = 2000 
GROUP BY state;
```

We can name columns something else... and then we can groupby multiple columns and order by a specific column in ascending order. 
```SQL
SELECT state, yr, sum(freq) AS num_johns
FROM mydatabase WHERE name = 'John'
GROUP BY state, yr
ORDER BY yr ASC;
```

It's easy to write illegal statements with `GROUP BY`...
```SQL
SELECT count(*), yr, state FROM mydatabase GROUP BY state;
```
... like this one. If you `SELECT` something, it needs to be aggregated in some way or in the `GROUP BY` command. 

Instead, we could do a few things. We could apply an aggregation to `yr`, or we could drop it or something. 

If we wanted to query how many different female names there were per state per year, we could ask...
```sql
SELECT yr, state, count(*) FROM mydatabase
WHERE gender = 'F'
GROUP BY yr, state;
```

If we want to find the names that have at least 1000 people in the year 2000, we need a subquery. We could ask...
```SQL
SELECT name, sum(freq) AS num_people
FROM mydatabse
WHERE yr = 2000
GROUP BY name;
```
... but this only gets us halfway. 

We really need to ask...
```sql
WITH num_names_in_2000 AS (
	SELECT name, sum(freq) AS num_people FROM mydatabase 
	WHERE yr = 2000 
	GROUP BY name )
	
	SELECT 
	name, num_people FROM num_names_in_2000 WHERE num_people >= 1000 			ORDER BY num_people DESC;

```

## Joins
We can join two parts of a table into a new table using `JOIN`. 

```sql
SELECT mydatabase.*, othertable.othercolumn 
FROM mydatabase LEFT JOIN othercolumn
ON mydatabase.column1 = othercolumn.column1
```

# Lecture - intro to [[fake job classifier]]
- Requirements
	- Access data in a SQL database
	- Build and employ a classification model
	- Optional: Publish an app with interactive visualizations
- Data
	- Can come from pre-selected dataset, data from scraping, data from an API or data from a cloud-hosted data warehouse
- Using SQL
	- In order to satisfly this req, we have to either store our data locally on a PostgreSQL database, extract and transform data in a cloud-hosted data warehouse before downloading it, or we can complete all of the four challenges in the project 3 starter folder. 
- Supervised learning
	- We need to have a discrete target variable / label. This variable could be [[binary]], or it could have multiple classes. 