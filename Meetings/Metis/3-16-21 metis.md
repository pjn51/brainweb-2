 # Metis Notes for Tuesday, March 16th
`LINKS:` [[metis week 11]]
`TAGS:` #meeting/career

---
Today is dedicated to a mock technical interview. 

## [[statistics]] and [[probability]] Questions
> What is a right / positive skewed distribution? How does mean compare to median value here? What's an example of a right positive skewed dist?

A right skewed distribution (see [[data and distributions]]) has a much longer tail on the right side when viewed as a histogram. The mean would be higher than the median value here. An example of this would be deaths per airline flight. The vast majority of observations would be at zero, but there would be some observations in the thousands (morbid, but for some reason that’s what came to mind…).

> What is a p-value?

A p-value is a statistical measure of certainty that the observed relationship that we’re seeing is in fact not due to random chance. A cutoff point, usually 0.05, is chosen, and a p-value below 0.05 indicates there is a less than 5% chance that the observed relationship is due to chance. 

> In any 15 min interval, there is a 20% probability that we will see a dog walk by. What is the probability of seeing at least one in an hour?

My ==wrong== answer: Since these 'events' are independent of one another, we would add up the probabilities. So after an hour (four 15-minute time periods) we would have an 80% chance of seeing at least one dog. 

The right answer: We take the probability of *not* seeing a dog, and multiply it by the four periods. So $0.80^4$, which is 0.41, is our probability of never seeing a dog. Therefore, the probability that we see at least one dog is 0.59, or 59%. 

> If I build a [[linear regression]] and then duplicate every [[data]] point and refit the model, what would happen to the model, R^2, and the p-values?

My ==wrong== answer: The model would perform better, since there are more observations to account for. The R^2 would increase, and the p-value would decrease. 

The right answer: Model fit wouldn't change, R^2 would stay the same, but the p-value would fall. 

> A regional chain wants to predict the number of people in a given store at a given time. They have hourly data for a week as well as the day of the week, current weather, local events, and the promotions. How would we approach the problem. 

First, I would clean the data (see [[data wrangling]]) and make sure there were no errors or outliers that would throw off the model. Then I would ask about seasonality and make sure this was a representative week. Then I would try to fit a linear regression using these features, and see how well I could account for new data, collected during another week if budget allowed. Based on the error of this model, I would perhaps move on to [[CART models]] such as random forest, but only if it was needed for the business case. 

Rita's addition: Poisson distribution [[GLM]] would be good for this. 

> Some students finish their projects early. Overall, 5% finish early, and instructors predict this correctly 90% of the time. However, instructors make false positive predictions 10% of the time. What is the prob that a student will actually finish early if the instructor thinks they will do so.

This is a case where we can use [[Bayes theorem]]. ==I had to look up bayes theorem, and I still didn't get this one right.==

> What is the central limit theorem?

==I didn't know this one.==

## [[SQL]] Questions
> What is the order of [[SQL]] operations? 

I believe it's `JOIN, FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY, LIMIT`. 

> Difference between WHERE and HAVING

> Differences between join clauses such as "inner," "outer," and "left"

Inner join keeps rows that are in both tables, outer keeps all rows. Left join keeps all found in the left table. 

> What will this query return?`SELECT Name FROM Customers WHERE ReferredBy != 2;`

It will return the names from the Customers table, but only the ones where they don't have a 2 in the ReferredBy column. 

> Write a SQL query to return the eighth highest salary from an employee salary table.

`SELECT Salary FROM Employee ORDER BY Salary DESC LIMIT 1 OFFSET 7;`

> Write a query to give the most recent pay amount for each employee.

`SELECT Name, Pay, MAX(DATE) OVER (PARTITION BY Name) FROM Employees;`

>  If we are examining the schema of a dating website, we might have a setup where there are USERS and LIKERS. Write a query that will count the number of liker's likers if the liker has at least one.

| Users | Likers |
| ----- | ------ |
| A     | B      |
| B     | C      |
| B     | D      |
| D     | E      | 

 ```sql
 SELECT l1.liker, COUNT(DISTINCT l2.liker) 
 FROM likes AS l1 
 INNER JOIN likes AS l2 
 ON l1.user = l2.liker
 ```
 
 > If we have a users table with id, name, timestamp, and city_id, and device. And if we have a user_comments table with id, body, and timestamp, how can we make a histogram of the number of comments per user in the month of jan 2019. 

```sql
WITH hist as (
SELECT users.user_id, COUNT(user_comments.user_id) AS num_comments
FROM users LEFT JOIN user_comments 
	ON users.id = user_comments.user_id
GROUP BY user_id
HAVING MONTH(created_at)= 1 & YEAR(created_at) = 2019 )

SELECT comment, count, COUNT(*) as frequency FROM HIST group by 1;
```

## Project Questions
When describing a project that didn't go well, focus on what you'll do next time. Also make sure to stay on your timeline when describing the workflow! It gets confusing when you skip around too much.

When describing a project, make sure to draw out the business use case when that isn't obvious from your project's subject matter. Make sure that the tools that you used stand out when this is the case. 

## Data structures, algorithms, and metrics
> Describe the steps of the [[KNN]] classifier algorithm.

KNN calculates the [[euclidian distance]] between the unknown-class point and the known points, and then based on the value for k that you’ve given the model, it classifies the point based on the most common class of the k points that are nearest to the point in question. This algorithm must perform as many calculations as there are data points in the training data, since we have to determine the distance between each new point and each known point. We have to scale our features so that no feature is weighed more heavily in the distance calculation.

> What do overfitting and underfitting mean, and how can we address each issue?

A model that is underfit is failing to notice patterns that actually exist in the data. A model that is overfit is inventing new patterns that do not actually exist in the data. In order to prevent overfitting, we can incorporate model complexity into our cost [[function]] so that the model ‘prefers’ a less complex model. To deal with underfitting, we can use a model that is more flexible and / or more complex. See [[bias-variance tradeoff]].

> What is the difference between a parameter and a hyperparameter? Give an example of each. 

Parameters are set automatically by a model, while we set [[hyperparameters]], which have an effect on the parameters of the model. For example, with linear regression, a parameter would be a variable coefficient, and a hyperparameter would be $\lambda$, the regularization strength. 

> You have to create a ML model to determine if credit card transactions are fraudulent. There are 100,000 transactions, and 3,000 are fraudulent. What problems do we expect and how would we deal with them?

One big problem would be a [[class imbalance]]. This means that we have to use metrics besides [[accuracy]], as we would get 97% accuracy from saying nothing is fraudulent. Therefore we can use the area under the ROC AUC curve. We should also try and balance the classes. We can either undersample the majority class, or we can oversample the minority class. If we want to oversample, we can either create new observations, or we can duplicate observations in the minority class. 

> 100 x 100 pixel image. Dimensions of input and output layer?

I believe that the input layer would have dimensions 10,000 x 1, and output would be 3 x 1. Since the [[neural network]] is densely connected, we flatten the image into a 1D vector before putting it in. 

> Which is faster, KNN or [[logistic regression]]? 

[[KNN]] is slower when predicting than [[logistic regression]]. As for train time, KNN is faster because it doesn't really have to train at all, it just calculates distances when new points arrive. 