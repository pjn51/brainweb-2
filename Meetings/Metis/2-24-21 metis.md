# Metis Notes for Wed, Feb 24th
`LINKS:` [[metis week 8]]
`TAGS:` #meeting/career

---
Today we're taking part two of the [[unsupervised learning]] test, and then we have a lecture on time series. 

# Lecture - linear regression on time-series
We have to use time series analysis when we want to predict how something will change over time. We can't assume that there's independence between observations, since the time at which the observation occurs is obviously important here. Therefore, we can't think about each point as an independent 'firing' of the stochastic process, and we have to assume that observations contain information about other observations.

## Auto-regression
The setup is similar to a normal [[linear regression]] - we have some features and we want to predict a target. We can use an *auto-regressive* model for this. An auto-regressive model is a regression where one or more previously observed target feature is used as a feature for the next prediction.

If we're trying to predict a movie's revenue after release, we might use the revenue from the previous week as a predictive feature for next week's revenue.

When we add an auto-regressive feature, we have to take *lag* into account. Lag is the amount of time steps between the present prediction and the auto-regressive feature being taken into account. We could use a 1-step lag, or a feature from further ago, such as 2 or 3 time steps ago. Or we could use a bunch of them at once. One thing to take into account is that we can only use a feature from 5 time steps ago after 5 time steps of [[data]]. 

Overall, we should just think of lagged features as another potential source of information that we can combine with other modeling methods. Auto-regressive models are best at short term forecasting, and get less and less predictive as our prediction horizon expands.

One important difference between linear regression and time series analysis is the way that we perform train-test splitting. We can't just randomly choose 20% as a holdout like we would for regression, we have to try and simulate what a real world deployment would look like. We should use the earliest 80% as train, and the latest 20% as test. 

If we want to examine how our model would perform as we get access to more and more data, we can do a sort of cross [[model validation|validation]] by changing the ratio of train and test above.

Another thing to think about is that when we add more lagged features, we improve train performance but we can risk overfitting. Basically we move towards variance on the [[bias-variance tradeoff]] spectrum.

## Moving average models
Another kind of time series model is called the *moving average model*, or MA. This helps to deal with noise present in the time data. It decreases variance at the cost of more bias. For MA to wkr, all the change to the value should be coming from the stochastic part, not from features included in the model. Basically, the mean and [[standard deviation]] of the time series have to be stationary for MA to work.

## Luck factor
There's a certian amount of noise that our model is unaware of. Since we can't predict the variation caused by this nouse, we have to implement some kind of luck factor that represents the degree to which the model has predicted the noise correctly.

The noise isn't really random, but we just don't know what's causing it. We model the noise as randomness just to get an approximation.

## ARMA
We can combine auto-regression with a MA model. If all we know is the previous target, we can use that as a lag feature and use the MA model to try and get the noise under control.

## Non-stationary time series
Most time series aren't really stationary, and there are methods for making them stationary so that we can use MA models with them. We can apply *differencing* to make time series with steadily rising means stationary. 

First order differencing is just subtracting the previous value, second order is looking at differences between the differences, and so on. We usually have to go to second order differencing for the time series to become stationary. 

We can also apply seasonal differencing to account for seasonal trends, depending on the use-case.

## ARIMA
If we do differencing, and then fit an auto-regressive model or a MA model, or an ARMA model, we're doing ARIMA. This is basically a special case for linear regression with some pre-processing thrown in there. ARIMA isn't really *its own thing,* perse. Just know that these sorts of tools exist for linear regression when dealing with time series information that's noisy.